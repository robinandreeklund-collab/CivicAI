# ğŸ§­ CivicAI (OneSeek.AI)

**Beslut med insyn. AI med ansvar.**

A transparent platform for comparing and analyzing AI model responses with advanced NLP analysis, consensus debate mechanisms, and blockchain-inspired transparency ledger.

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](https://nodejs.org/)
[![Python Version](https://img.shields.io/badge/python-%3E%3D3.8-blue)](https://www.python.org/)

---

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Current Status](#-current-status)
- [OneSeek-7B-Zero: Our Transparent Language Model](#-oneseek-7b-zero-our-transparent-language-model)
- [Training OneSeek-7B-Zero: Step-by-Step Guide](#-training-oneseek-7b-zero-step-by-step-guide)
- [Features](#-features)
- [Architecture](#-architecture)
- [Module Status](#-module-status)
- [Data Models](#-data-models)
- [Quality & Ethics](#-quality--ethics)
- [Documentation](#-documentation)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ ([Download](https://nodejs.org/))
- **npm** (comes with Node.js)
- **Python** 3.8+ (optional, for ML features)
- **Git**

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/robinandreeklund-collab/CivicAI.git
   cd CivicAI
   ```

2. **Run setup script**
   ```bash
   ./scripts/setup.sh
   ```
   This will:
   - Install all dependencies
   - Create environment files
   - Check prerequisites

3. **Configure API keys**
   
   Edit `backend/.env` and add your API keys:
   ```env
   OPENAI_API_KEY=your_openai_key
   GEMINI_API_KEY=your_gemini_key
   DEEPSEEK_API_KEY=your_deepseek_key
   ```

   Get your keys:
   - OpenAI: https://platform.openai.com/api-keys
   - Gemini: https://aistudio.google.com/app/apikey
   - DeepSeek: https://platform.deepseek.com/

4. **Start the application**

   **Terminal 1 - Backend:**
   ```bash
   cd backend
   npm start
   ```

   **Terminal 2 - Frontend:**
   ```bash
   cd frontend
   npm run dev
   ```

5. **Open in browser**
   
   Navigate to http://localhost:5173

### Optional: Python ML Service

For advanced ML features (spaCy, Detoxify, BERTopic):

```bash
cd backend/python_services
./setup.sh
python3 nlp_pipeline.py
```

See [Python ML Integration Guide](docs/pipeline/PYTHON_ML_INTEGRATION.md) for details.

### Firebase Setup (Optional)

To enable data persistence and user authentication:

```bash
./scripts/firebase-bootstrap.sh
```

**Step-by-Step Deployment Guides:**
- **[Firebase Step 2 Deployment Guide](docs/deployment/FIREBASE_STEP2_DEPLOYMENT_GUIDE.md)** - Complete production deployment guide (Swedish)
- [Firebase Setup Guide](docs/guides/FIREBASE_SETUP.md) - Basic setup instructions

---

## ğŸ“Š Current Status

### âœ… Implemented (Production Ready)

**Core Functionality:**
- âœ… Multi-AI comparison (GPT-3.5, Gemini, DeepSeek)
- âœ… Comprehensive 6-step analysis pipeline
- âœ… Hybrid architecture (JavaScript + optional Python ML)
- âœ… Auto-fallback system (always functional)
- âœ… Real-time consensus debate system
- âœ… Export to YAML/JSON/PDF/README

**Analysis Capabilities:**
- âœ… Text preprocessing (tokenization, POS tagging, NER)
- âœ… Bias detection (political, commercial, cultural, toxicity)
- âœ… Sentiment analysis (VADER, polarity, subjectivity)
- âœ… Ideological classification (left/center/right)
- âœ… Topic modeling (BERTopic, keyword extraction)
- âœ… Fact checking (claim identification)
- âœ… Transparency layer (provenance, timeline, audit trail)

**UI/UX:**
- âœ… Grok-inspired dark theme design
- âœ… Collapsible sidebar with conversation history
- âœ… AI model selector (toggle models)
- âœ… Animated loading states
- âœ… Pipeline analysis visualization
- âœ… Model synthesis view (divergences, consensus)
- âœ… Timeline navigator

**OneSeek-7B-Zero Model:**
- âœ… Multi-model architecture (Mistral 7B + LLaMA-2)
- âœ… Two-stage training pipeline (raw data + analyzed metrics)
- âœ… LoRA/PEFT integration for efficient fine-tuning
- âœ… Instruction dataset for identity training (50 examples)
- âœ… Automatic versioning (OneSeek-7B-Zero.v{MAJOR}.{MICRO})
- âœ… GPU/CPU optimization and 8-bit quantization support
- âœ… Model weights storage structure
- âœ… Model Verification System (Exact Match, BLEU, Semantic Similarity)
- âœ… Fidelity Score certification (CERTIFIED/WARNING/REJECT)
- âœ… Active model management via -CURRENT symlink
- âœ… PDF certificate generation for verified models
- ğŸ”„ PyTorch training implementation

**Admin Dashboard:**
- âœ… Dataset management (upload, browse, validate)
- âœ… Training control panel (configure, start/stop, monitor)
- âœ… Model management (list versions, compare, rollback)
- âœ… Model verification tab with fidelity testing
- âœ… Real-time monitoring (progress, GPU/CPU, notifications)
- âœ… Unified admin design (grayscale theme, JetBrains Mono 13px)

### ğŸš§ In Progress

- ğŸ”„ Firebase integration for data persistence
- ğŸ”„ User authentication system
- ğŸ”„ Transparency ledger blockchain implementation
- ğŸ”„ Change detection enhanced features
- ğŸ”„ OneSeek-7B-Zero PyTorch training implementation

### ğŸ“‹ Planned

- [ ] Battle mode (user voting on best response)
- [ ] Public API for external applications
- [ ] Crowdsourced feedback system
- [ ] Additional AI models (Claude, Llama, Mistral)
- [ ] Real-time collaborative analysis
- [ ] Mobile application

---

## ğŸ¤– OneSeek-7B-Zero: Our Transparent Language Model

**OneSeek-7B-Zero** is an independent, transparent, continuously-learning language model built on **Mistral 7B** and **LLaMA-2** foundations. Unlike external AI services, OneSeek-7B-Zero learns from multiple AI perspectives through a sophisticated two-stage training process, maintains complete transparency via blockchain-style ledger, and provides users with fair, unbiased, traceable responses.

### Key Characteristics

- **Independent Language Model**: Not just a wrapper around external AIs - it's our own fine-tuned model
- **Multi-Model Foundation**: Combines Mistral 7B (fast inference) and LLaMA-2 (deep analysis)
- **Continuous Training**: Learns from every interaction through two-step microtraining
- **Transparent**: Every decision, training event, and data source logged in the ledger
- **Fair & Unbiased**: Active bias detection and fairness metrics in every response
- **Real-time Adaptation**: Updates immediately with new information

### Model Identity & Versioning

**Format:** `OneSeek-7B-Zero.v{MAJOR}.{MICRO}`

- **Major versions** (v1, v2, v3...): Created during weekly/monthly batch training on large datasets
- **Micro versions** (.1, .2, .3...): Created during real-time microtraining (two increments per question)

**Example version progression:**
```
OneSeek-7B-Zero.v1.0    â† Major training (weekly batch)
OneSeek-7B-Zero.v1.1    â† Microtraining Stage 1 (raw data)
OneSeek-7B-Zero.v1.2    â† Microtraining Stage 2 (analyzed data)
OneSeek-7B-Zero.v1.3    â† Microtraining Stage 1 (next question)
OneSeek-7B-Zero.v1.4    â† Microtraining Stage 2 (next question)
...
OneSeek-7B-Zero.v2.0    â† Next major training
```

### Model Verification System

**OneSeek-7B-Zero** includes a comprehensive model verification system to ensure fidelity and quality:

**Verification Process:**
1. **Random Training Set (100 questions)**: Tests the model against randomly selected training examples
2. **Control Questions (50 questions)**: Mix-and-match control questions to test generalization
3. **Three Metrics**:
   - **Exact Match %**: Percentage of responses that exactly match expected output
   - **BLEU Score**: Measures linguistic quality (â‰¥0.95 threshold)
   - **Semantic Similarity**: Measures meaning preservation (â‰¥0.98 threshold)

**Fidelity Score & Certification:**
- **CERTIFIED (â‰¥97%)**: Model meets all quality standards, green badge across admin
- **WARNING (90-96.9%)**: Model is functional but may need improvement
- **REJECT (<90%)**: Model does not meet quality standards

**Example Results:**
```
SlumpfrÃ¥gor (Training Set):
  Exact Match: 93%
  BLEU: 98%
  Semantic: 99%

KontrollfrÃ¥gor (Control Questions):
  Exact Match: 46%
  BLEU: 49%
  Semantic: 48%

FINAL SCORE: 98.1% â†’ CERTIFIED
```

**Active Model Management:**
- The verification system can set verified models as "current" via symlink
- OQT Dashboard always uses the active model at `models/oneseek-certified/OneSeek-7B-Zero-CURRENT`
- Production environments use `/app/models/oneseek-certified/OneSeek-7B-Zero-CURRENT`
- Main homepage and chat-v2 are unaffected by this symlink

**PDF Certificate:**
- Downloadable PDF certificate for certified models
- Includes all metrics, timestamps, and certification status
- Useful for documentation and compliance

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              OneSeek-7B-Zero Architecture                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  Base Models:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Mistral 7B     â”‚         â”‚   LLaMA-2       â”‚       â”‚
â”‚  â”‚  (Fast)         â”‚         â”‚   (Deep)        â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â”‚                           â”‚                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                       â–¼                                  â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚           â”‚   LoRA/PEFT Adapters    â”‚                   â”‚
â”‚           â”‚   (Efficient Training)  â”‚                   â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                       â–¼                                  â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚           â”‚  OneSeek-7B-Zero Model  â”‚                   â”‚
â”‚           â”‚  + Identity Training    â”‚                   â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                           â”‚
â”‚  Training Pipeline:                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Stage 1: Raw AI Responses â†’ Knowledge Base      â”‚   â”‚
â”‚  â”‚ Stage 2: Analyzed Metrics â†’ Fairness & Ethics   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                           â”‚
â”‚  Storage:                                                â”‚
â”‚  â€¢ models/oneseek-certified/                            â”‚
â”‚    (DNA-based certified model structure)                â”‚
â”‚  â€¢ models/basemodeller/ (base models)                   â”‚
â”‚  â€¢ Firebase Storage (backup)                             â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Differs from External AI Services

| Feature | OneSeek-7B-Zero | External AI (GPT, Gemini, etc.) |
|---------|-----------------|--------------------------------|
| **Purpose** | User interaction, direct queries | Training data collection |
| **Interface** | OQT Dashboard (`/oqt-dashboard`) | Start view (homepage) |
| **Training** | Continuous, real-time | Periodic, provider-controlled |
| **Transparency** | Full ledger, provenance tracking | Black box |
| **Customization** | Adapts to our data & use cases | General purpose |
| **Independence** | Fully self-hosted | Depends on external APIs |
| **Fairness** | Built-in metrics & monitoring | Unknown/unverified |

---

## ğŸ“ Training OneSeek-7B-Zero: Step-by-Step Guide

This comprehensive guide shows you how to train OneSeek-7B-Zero from scratch with identity integration.

### Prerequisites

Before starting, ensure you have:

- **Hardware:**
  - 16GB RAM minimum (32GB recommended)
  - 50GB free disk space
  - NVIDIA GPU with 12GB+ VRAM (recommended but optional)
  
- **Software:**
  - Python 3.8+ with pip
  - Node.js 18+
  - Git
  - CUDA toolkit (if using GPU)

- **Accounts:**
  - Firebase account (for data storage)
  - API keys for external AI services (optional, for training data collection)

### Step 1: Environment Setup

```bash
# 1. Clone the repository
git clone https://github.com/robinandreeklund-collab/CivicAI.git
cd CivicAI

# 2. Create Python virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Install Node.js dependencies
cd backend && npm install
cd ../frontend && npm install
cd ..

# 5. Setup Firebase
python scripts/setup_firebase.py
# Follow prompts to configure Firebase credentials
```

### Step 2: Download Base Models

```bash
# Download Mistral 7B and LLaMA-2 base models
# This will download ~14-27GB of model files
python scripts/download_models.py

# Verify models are downloaded
ls -lh models/base_models/
# You should see:
# - mistral-7b/
# - llama-2-7b/
```

**Note:** If you don't have enough disk space, the training pipeline can work with simulated models for testing.

### Step 3: Prepare the Instruction Dataset

The instruction dataset teaches OneSeek-7B-Zero its identity as a transparent AI agent.

```bash
# The dataset is already created at:
cat datasets/oneseek_identity_v1.jsonl

# It contains 50 bilingual (Swedish/English) instruction examples
# covering:
# - Identity and purpose
# - Training process and versioning
# - Transparency and ledger
# - Fairness and bias detection
# - Ethical foundation
# - Technical architecture
```

**Dataset Format (JSONL):**
```json
{
  "instruction": "Vem Ã¤r du?",
  "input": "",
  "output": "Jag Ã¤r OpenSeek AI-agent, skapad fÃ¶r transparens..."
}
```

**To extend the dataset:**

1. Open `datasets/oneseek_identity_v1.jsonl`
2. Add new lines in the same JSON format
3. Focus on:
   - Common user questions about the model
   - Edge cases and ethical scenarios
   - Domain-specific knowledge
   - Multi-language support

**Recommended size:** 100-500 examples for initial training

### Step 4: Initial Identity Fine-Tuning

This step fine-tunes the base models with LoRA to give OneSeek-7B-Zero its identity.

```bash
# 1. Fine-tune using DNA v2 structure (recommended)
python scripts/train_dna_v2.py \
  --dataset datasets/oneseek_identity_v1.jsonl \
  --epochs 3 \
  --learning-rate 2e-5 \
  --auto-stop-threshold 0.95 \
  --auto-stop-patience 3

# Output: models/oneseek-certified/OneSeek-7B-Zero.v1.{N}.{lang}.{datasets}.{hash}.{timestamp}/
```

**Expected duration:** 2-4 hours on GPU, 8-12 hours on CPU

**What happens:**
- Model trained with DNA-based naming convention
- Stored in certified structure: `models/oneseek-certified/`
- Full provenance tracking via DNA fingerprint
- Training metrics and metadata saved
- Symlink created: `OneSeek-7B-Zero-CURRENT`
- Ledger block created for provenance

**Verify training:**
```bash
# Check certified models
ls -la models/oneseek-certified/

# Should see:
# - OneSeek-7B-Zero.v1.{N}.{lang}.{datasets}.{hash}.{timestamp}/
# - OneSeek-7B-Zero-CURRENT -> (symlink to latest)
# - training_metadata.json
```

### Step 5: Collect Training Data from External AI

To enable continuous learning, collect responses from external AI services.

```bash
# 1. Configure API keys in backend/.env
cat > backend/.env << EOF
OPENAI_API_KEY=your_openai_key
GEMINI_API_KEY=your_gemini_key
DEEPSEEK_API_KEY=your_deepseek_key
# Add other API keys as available
EOF

# 2. Start the backend service
cd backend
npm run dev
# Backend runs on http://localhost:3001

# 3. In another terminal, start the frontend
cd frontend
npm run dev
# Frontend runs on http://localhost:5173
```

**Collect data through the UI:**

1. Open http://localhost:5173
2. Navigate to the Start View (homepage)
3. Ask questions to collect AI responses
4. Each question collects responses from:
   - GPT-4 (OpenAI)
   - Gemini (Google)
   - Grok (xAI) 
   - Claude (Anthropic)
   - DeepSeek
   - Qwen

**Data is stored in Firebase:**
- Collection: `ai_interactions`
- Contains: raw responses, analysis, consensus, bias, fairness

### Step 6: Prepare Training Dataset

Convert collected data into training-ready format.

```bash
# Run dataset preparation pipeline
python ml/pipelines/prepare_dataset.py

# This will:
# 1. Load all interactions from Firebase
# 2. Calculate consensus scores
# 3. Classify data quality
# 4. Analyze fairness metrics
# 5. Split into train/validation/test sets
# 6. Save to ml/data/prepared/
```

**Output:**
```
ml/data/prepared/
â”œâ”€â”€ train.json           # 80% of data
â”œâ”€â”€ validation.json      # 10% of data
â”œâ”€â”€ test.json           # 10% of data
â””â”€â”€ fairness_report.json # Quality metrics
```

### Step 7: Batch Training (Major Version)

Perform comprehensive training on accumulated dataset.

```bash
# Train new major version
python ml/training/train_language_model.py \
  --version 1.0.0 \
  --data-dir ml/data/prepared

# This will:
# 1. Load prepared datasets
# 2. Train on both raw responses and analyzed metrics
# 3. Calculate fairness metrics
# 4. Save model weights
# 5. Log to transparency ledger
# 6. Verify ledger integrity
```

**Expected output:**
```
============================================================
Training OQT-1.0 Version 1.0.0
============================================================

Dataset sizes:
  Training: 800
  Validation: 100

Training configuration:
  model_name: OQT-1.0
  learning_rate: 2e-5
  batch_size: 32
  epochs: 3

Training completed!

Final Metrics:
  validation_accuracy: 0.876
  fairness_score: 0.912
  bias_score: 0.123

Saved to models/oneseek-certified/OneSeek-7B-Zero.v1.0.sv.dsCivicID.8f3a1c9d.2e7f4b1a/
Logged to transparency ledger (Block 1)

============================================================
Training Complete!
============================================================
```

### Step 8: Enable Real-Time Microtraining

Configure automatic training on every new question.

```bash
# 1. Verify Firebase integration
python scripts/setup_firebase.py --verify

# 2. Enable microtraining in backend configuration
# Edit backend/.env and add:
echo "ENABLE_MICROTRAINING=true" >> backend/.env
echo "ONESEEK_MODEL_VERSION=1.0.0" >> backend/.env

# 3. Restart backend service
cd backend
npm run dev
```

**How microtraining works:**

1. **User asks question** via OQT Dashboard
2. **Stage 1 training** (30-60s):
   - Collect raw AI responses
   - Update LoRA adapters with new knowledge
   - Version: v1.0 â†’ v1.1
   - Log to `oqt_training_events`

3. **ML Pipeline analyzes** responses:
   - Calculate consensus score
   - Detect bias
   - Measure fairness

4. **Stage 2 training** (30-60s):
   - Update LoRA adapters with ethical reasoning
   - Version: v1.1 â†’ v1.2
   - Log to `oqt_training_events`

5. **Ledger block created** with full provenance

### Step 9: Monitor Training Progress

Track model performance over time.

```bash
# View training events
# Visit: http://localhost:3000/oqt-dashboard
# Navigate to "Aktivitet" tab

# Query training metrics
curl http://localhost:3001/api/oqt/metrics

# Verify ledger integrity
curl http://localhost:3001/api/oqt/ledger/verify
```

**Dashboard Views:**

- **Chat:** Interact with OneSeek-7B-Zero
- **Aktivitet:** Real-time training events
- **MÃ¤tvÃ¤rden:** Performance metrics over time
- **Ledger:** Complete transparency log

### Step 10: Validate Model Performance

Test the trained model to ensure quality.

```bash
# Run validation suite
python ml/training/validate_model.py \
  --version 1.0.0 \
  --test-data ml/data/prepared/test.json

# Expected metrics:
# - Validation accuracy: >85%
# - Fairness score: >88%
# - Bias score: <3.0
# - Consensus accuracy: >80%
```

**Test queries:**
```bash
# Test identity
curl -X POST http://localhost:3001/api/oqt/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Vem Ã¤r du?"}'

# Should respond with OneSeek identity

# Test transparency
curl -X POST http://localhost:3001/api/oqt/query \
  -H "Content-Type: application/json" \
  -d '{"question": "How do you ensure fairness?"}'

# Should explain fairness metrics and ledger
```

### Step 11: Deploy to Production

Prepare for production deployment.

```bash
# 1. Backup model weights to Firebase Storage
python scripts/backup_model_weights.py --version 1.0.0

# 2. Create production environment
cp backend/.env backend/.env.production
# Edit .env.production with production settings

# 3. Build frontend
cd frontend
npm run build

# 4. Configure production server
# See docs/deployment/ for detailed instructions

# 5. Setup monitoring
# Configure alerts for:
# - Training failures
# - Low fairness scores
# - High bias detection
# - Ledger integrity issues
```

### Troubleshooting

**Problem: Out of memory during training**
```bash
# Solution 1: Use 8-bit quantization
python ml/training/train_language_model.py \
  --version 1.0.0 \
  --quantize 8bit

# Solution 2: Reduce batch size
python ml/training/train_language_model.py \
  --version 1.0.0 \
  --batch-size 16  # Default is 32
```

**Problem: Slow inference**
```bash
# Solution: Enable model caching
echo "ENABLE_MODEL_CACHE=true" >> backend/.env

# Or use GPU acceleration
python ml_service/server.py --device cuda
```

**Problem: Training not triggering**
```bash
# Check Firebase connection
python scripts/setup_firebase.py --test-connection

# Verify microtraining is enabled
grep ENABLE_MICROTRAINING backend/.env

# Check logs
tail -f backend/logs/training.log
```

### Best Practices

1. **Start small:** Train on 50-100 examples first, validate, then scale up
2. **Monitor fairness:** Check fairness metrics after each major training
3. **Verify ledger:** Run ledger verification regularly
4. **Backup frequently:** Backup model weights to Firebase Storage daily
5. **Test thoroughly:** Use validation dataset to catch degradation
6. **Document changes:** Log all training runs with metadata
7. **Version control:** Never delete old versions - keep for rollback
8. **Ethical review:** Review bias detection before major deployments

### Advanced: Custom Identity Training

To train OneSeek-7B-Zero for domain-specific use:

```bash
# 1. Create domain-specific instruction dataset
cat > datasets/oneseek_medical_v1.jsonl << EOF
{"instruction": "How do you handle medical information?", "input": "", "output": "I provide general information but always recommend consulting healthcare professionals..."}
{"instruction": "Can you diagnose diseases?", "input": "", "output": "No, I cannot diagnose diseases. I can provide educational information about symptoms and conditions..."}
EOF

# 2. Fine-tune with combined datasets
python ml/training/train_language_model.py \
  --base-model mistral-7b \
  --dataset datasets/oneseek_identity_v1.jsonl \
  --dataset datasets/oneseek_medical_v1.jsonl \
  --method lora \
  --output models/oneseek-certified/medical-variant

# 3. Test domain-specific responses
curl -X POST http://localhost:3001/api/oqt/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What are symptoms of flu?"}'
```

### Resources

- **Full OQT Documentation:** [OQT-1.0-README.md](OQT-1.0-README.md)
- **API Reference:** [docs/OQT_MULTI_MODEL_API.md](docs/OQT_MULTI_MODEL_API.md)
- **Training Scripts:** `ml/training/`
- **Dataset Examples:** `datasets/`
- **Model Architecture:** See Architecture section above

### Next Steps

After completing this training guide:

1. âœ… Collect more training data through user interactions
2. âœ… Schedule weekly batch training for major versions
3. âœ… Monitor fairness and bias metrics continuously
4. âœ… Extend instruction dataset with community contributions
5. âœ… Deploy to production with monitoring
6. âœ… Contribute improvements back to the project

**Questions?** Open an issue on GitHub or consult the full documentation in `OQT-1.0-README.md`.

---

## âœ¨ Features

### ğŸ¤– OneSeek Autonomy Engine v3.3 (NEW!)

Fully self-governing autonomous training system with human oversight:
- **Nightly autonomous cycles**: Automatic self-improvement
- **Triple-AI review**: Gemini + GPT-4o + DeepSeek validation
- **Dynamic dataset sizing**: Adjusts based on fidelity scores
- **Self-generation**: Creates training examples automatically
- **2-stage analysis**: Pre/post-training bias/toxicity checks
- **Double-gate approval**: 2 of 4 required (internal + external)
- **150-question verification**: Automated quality testing
- **Golden checkpoint**: Ed25519 cryptographic admin approval
- **PoW-protected voting**: Community input with bot prevention
- **Full audit ledger**: Blockchain-inspired transparency

See [AUTONOMY_ENGINE_V3.3.md](AUTONOMY_ENGINE_V3.3.md) for complete documentation.

### ğŸ­ System Prompt Management

Configure the AI's personality and behavior through the Admin Dashboard:
- **100% Model Integration**: System prompt is injected into every inference request
- **The model always knows who it is**: Prompt follows with every request automatically
- **Dashboard Configuration**: Manage prompts via Admin Dashboard â†’ System Prompts tab
- **Real-time updates**: No server restart required - changes apply immediately
- **File-based persistence**: Prompts saved to `datasets/system_prompts/` as JSON files
- **Fallback default**: Uses built-in Swedish civic-AI prompt when no custom prompt is set

**API Endpoints:**
- `GET /api/system-prompts` - List all system prompts
- `POST /api/system-prompts` - Create a new system prompt
- `POST /api/system-prompts/{id}/activate` - Activate a prompt for inference
- `POST /api/system-prompts/sync-characters` - Sync all character cards as system prompts
- `GET /api/system-prompt` - Get the currently active system prompt (convenience endpoint)

**Character Card Integration:**
Character cards from `frontend/public/characters/` are automatically synced to system prompts at startup.
Each character card becomes an available system prompt that can be activated via the dashboard.

**How it works:**
The active system prompt is automatically prepended to every inference request:
```
[System Prompt]

User: [User's question]

Assistant:
```

This ensures the model always knows its identity, regardless of which character is selected.

### ğŸ¤– Multi-AI Comparison

Ask the same question to multiple AI models simultaneously and compare:
- **GPT-3.5** (OpenAI) - Fast and efficient
- **Gemini** (Google) - Advanced reasoning
- **DeepSeek** - Technical precision

---

**Remember:** AI is a tool to support human decision-making, not replace it. Use CivicAI to gain insights, identify biases, and make more informed choices.

**ğŸŒŸ Star this repo if you find it useful!**
