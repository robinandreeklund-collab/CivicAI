#!/usr/bin/env python3
"""
OneSeek-7B-Zero Language Model Training
Batch training for the OneSeek-7B-Zero (formerly OQT-1.0) model
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# Add pipelines to path
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), 'pipelines'))

from transparency_ledger import TransparencyLedger


class OneSeekTrainer:
    """Trainer for OneSeek-7B-Zero language model"""
    
    def __init__(self, data_dir: str, model_dir: str, ledger_dir: str, language: str = 'en', external_model: str = None):
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.language = language
        self.external_model = external_model
        
        # Initialize transparency ledger
        self.ledger = TransparencyLedger(ledger_dir)
        
        # Training config
        # Note: base_models should be set dynamically by the caller
        # Default values are placeholders and should be overridden
        self.config = {
            'model_name': 'OneSeek-7B-Zero' if language == 'en' else 'OneSeek-7B-Zero-SV',
            'legacy_name': 'OQT-1.0',  # For backward compatibility
            'language': language,
            'external_model': external_model,
            'architecture': 'transformer',
            'base_models': [],  # Must be set by caller - no hardcoded defaults
            'learning_rate': 2e-5,
            'batch_size': 32,
            'epochs': 3,
            'warmup_steps': 500,
            'use_lora': True,
            'lora_rank': 8,
            'lora_alpha': 32
        }
    
    def load_training_data(self) -> Dict:
        """Load prepared training datasets"""
        datasets = {}
        
        for split in ['train', 'validation', 'test']:
            file_path = self.data_dir / f"{split}.json"
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    datasets[split] = json.load(f)
                print(f"Loaded {len(datasets[split])} {split} samples")
            else:
                print(f"Warning: {split} dataset not found at {file_path}")
                datasets[split] = []
        
        return datasets
    
    def calculate_dataset_hash(self, datasets: Dict) -> str:
        """Calculate hash of all training data for provenance"""
        import hashlib
        combined_data = json.dumps(datasets, sort_keys=True)
        return hashlib.sha256(combined_data.encode()).hexdigest()
    
    def train_model(self, datasets: Dict, version: str, run_id: str = None) -> Dict:
        """
        Train model - uses PyTorch if available, otherwise simulates
        
        Args:
            datasets: Training datasets
            version: Model version
            run_id: Training run ID for live metrics (optional)
            
        Returns:
            Training results with metrics
        """
        # Try to import PyTorch training module
        try:
            import sys
            from pathlib import Path
            
            # Add pytorch_trainer to path
            pytorch_trainer_path = Path(__file__).parent
            if str(pytorch_trainer_path) not in sys.path:
                sys.path.insert(0, str(pytorch_trainer_path))
            
            from pytorch_trainer import (
                verify_requirements,
                train_with_pytorch_lora,
                check_base_models
            )
            
            # Check if PyTorch training is available
            if verify_requirements():
                print("\n[INFO] PyTorch training environment detected!")
                print("   Using real LoRA/PEFT training...\n")
                
                # Get base models directory
                base_models_dir = self.model_dir.parent / 'base_models'
                
                # Check if base models exist
                available_models = check_base_models(base_models_dir)
                
                if available_models:
                    # Get selected base models from environment variable
                    import os
                    selected_base_models = None
                    base_models_env = os.environ.get('BASE_MODELS', '')
                    if base_models_env:
                        selected_base_models = [m.strip() for m in base_models_env.split(',') if m.strip()]
                    
                    # Use real PyTorch training with selected base models
                    result = train_with_pytorch_lora(
                        datasets=datasets,
                        version=version,
                        model_dir=self.model_dir,
                        base_models_dir=base_models_dir,
                        config=self.config,
                        selected_base_models=selected_base_models,
                        run_id=run_id
                    )
                    result['simulated'] = False
                    return result
                else:
                    print("\n[ERROR] No base models found.")
                    print(f"   Please download models to: {base_models_dir}")
                    return {'simulated': True, 'error': 'No base models found'}
            else:
                # Check what's missing
                try:
                    import torch
                    torch_available = True
                except ImportError:
                    torch_available = False
                    
                try:
                    import peft
                    peft_available = True
                except ImportError:
                    peft_available = False
                    
                print("\n[ERROR] PyTorch requirements not met.")
                return {
                    'simulated': True, 
                    'error': 'Requirements not met',
                    'torch_available': torch_available,
                    'peft_available': peft_available
                }
                
        except Exception as e:
            print(f"\n[ERROR] Could not use PyTorch training: {e}")
            import traceback
            traceback.print_exc()
            return {'simulated': True, 'error': str(e)}
    
    def simulate_training(self, datasets: Dict, version: str) -> Dict:
        """
        Simulate model training
        In production, this would use PyTorch/TensorFlow
        """
        print(f"\n{'=' * 70}")
        print(f"Training OneSeek-7B-Zero Version {version}")
        print(f"{'=' * 70}")
        
        train_size = len(datasets.get('train', []))
        val_size = len(datasets.get('validation', []))
        
        print(f"\nDataset sizes:")
        print(f"  Training: {train_size}")
        print(f"  Validation: {val_size}")
        
        print(f"\nTraining configuration:")
        for key, value in self.config.items():
            print(f"  {key}: {value}")
        
        print(f"\nStarting training...")
        
        # Simulate training metrics
        # In production, these would be actual training results
        metrics = {
            'training_loss': 0.342,
            'validation_accuracy': 0.876,
            'fairness_score': 0.912,
            'bias_score': 0.123,
            'consensus_accuracy': 0.854
        }
        
        fairness_metrics = {
            'demographic_parity': 0.945,
            'equal_opportunity': 0.928,
            'disparate_impact': 0.967
        }
        
        print(f"\nTraining completed!")
        print(f"\nFinal Metrics:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.3f}")
        
        print(f"\nFairness Metrics:")
        for key, value in fairness_metrics.items():
            print(f"  {key}: {value:.3f}")
        
        return {
            'metrics': metrics,
            'fairness_metrics': fairness_metrics
        }
    
    def save_model_version(self, version: str, datasets: Dict, results: Dict) -> Dict:
        """Save model version with complete metadata"""
        
        # Get actual trained base models from results, not hardcoded config
        trained_models = results.get('trained_models', {})
        if trained_models:
            # Use actual trained model names (already saved by pytorch_trainer.py)
            # Don't overwrite the correct metadata file that was already saved
            print(f"\n[INFO] PyTorch training already saved metadata with actual base models")
            print(f"[INFO] Skipping redundant metadata save to avoid overwriting correct data")
            
            # Return minimal version info for ledger
            model_version = {
                'version': version,
                'model_name': 'OneSeek-7B-Zero',
                'timestamp': datetime.now().isoformat(),
                'base_models': list(trained_models.keys()),
                'training_config': {
                    'dataset_size': len(datasets.get('train', [])),
                    'epochs': self.config['epochs'],
                    'batch_size': self.config['batch_size'],
                    'learning_rate': self.config['learning_rate'],
                },
                'metrics': results['metrics'],
                'fairness_metrics': results['fairness_metrics'],
                'provenance': {
                    'training_data_hash': self.calculate_dataset_hash(datasets),
                    'ledger_block_id': None,
                    'trainer': 'OneSeek-Training-Pipeline',
                    'notes': f'Batch training for version {version}'
                }
            }
        else:
            # Fallback for simulated training (no PyTorch)
            model_version = {
                'version': version,
                'model_name': 'OneSeek-7B-Zero',
                'legacy_name': 'OQT-1.0',
                'timestamp': datetime.now().isoformat(),
                'base_models': self.config['base_models'],
                'training_config': {
                    'dataset_size': len(datasets.get('train', [])),
                    'epochs': self.config['epochs'],
                    'batch_size': self.config['batch_size'],
                    'learning_rate': self.config['learning_rate'],
                    'use_lora': self.config['use_lora'],
                    'lora_rank': self.config['lora_rank'],
                    'lora_alpha': self.config['lora_alpha']
                },
                'metrics': results['metrics'],
                'fairness_metrics': results['fairness_metrics'],
                'provenance': {
                    'training_data_hash': self.calculate_dataset_hash(datasets),
                    'ledger_block_id': None,
                    'trainer': 'OneSeek-Training-Pipeline',
                    'notes': f'Batch training for version {version}'
                }
            }
            
            # Only save metadata file if PyTorch didn't already save it
            metadata_file = self.model_dir / f"oneseek-7b-zero-v{version}.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(model_version, f, indent=2)
            
            print(f"\nSaved model metadata to {metadata_file}")
        
        # Check if PyTorch weights were saved
        weights_file = self.model_dir / f"oneseek-7b-zero-v{version}.pth"
        if weights_file.exists():
            print(f"[SUCCESS] Model weights saved to {weights_file}")
        else:
            print(f"[INFO] Model weights will be saved to {weights_file} (PyTorch training required)")
        
        # Check if LoRA adapters were saved
        lora_path = self.model_dir.parent / 'lora_adapters' / f'oneseek-7b-zero-v{version}'
        if lora_path.exists():
            print(f"[SUCCESS] LoRA adapters saved to {lora_path}")
        
        return model_version
    
    def log_to_ledger(self, model_version: Dict) -> Dict:
        """Log training event to transparency ledger"""
        
        ledger_data = {
            'model_version': model_version['version'],
            'training_samples': model_version['training_config']['dataset_size'],
            'fairness_metrics': model_version['fairness_metrics'],
            'bias_metrics': {
                'bias_score': model_version['metrics']['bias_score']
            },
            'provenance': [
                f"training_data_hash:{model_version['provenance']['training_data_hash']}",
                f"trainer:{model_version['provenance']['trainer']}",
                f"timestamp:{model_version['timestamp']}"
            ],
            'training_config': model_version['training_config'],
            'metrics': model_version['metrics']
        }
        
        block = self.ledger.add_block('training', ledger_data, validator='OneSeek-Training-Pipeline')
        
        # Update model version with ledger reference
        model_version['provenance']['ledger_block_id'] = block['block_id']
        
        # Re-save metadata with ledger reference using new naming convention
        version = model_version['version']
        metadata_file = self.model_dir / f"oneseek-7b-zero-v{version}.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(model_version, f, indent=2)
        
        print(f"Logged to transparency ledger (Block {block['block_id']})")
        
        return block
    
    def train(self, version: str = '1.0.0'):
        """
        Main training pipeline
        
        Args:
            version: Semantic version for this model
        """
        print(f"\n{'=' * 70}")
        print(f"OneSeek-7B-Zero Training Pipeline")
        print(f"{'=' * 70}")
        
        # Load data
        print(f"\nStep 1: Loading training data...")
        datasets = self.load_training_data()
        
        if not datasets.get('train'):
            print("ERROR: No training data found!")
            print("Please run prepare_dataset.py first.")
            return
        
        # Train model
        print(f"\nStep 2: Training model...")
        results = self.train_model(datasets, version)
        
        # Save model version
        print(f"\nStep 3: Saving model version...")
        model_version = self.save_model_version(version, datasets, results)
        
        # Log to transparency ledger
        print(f"\nStep 4: Logging to transparency ledger...")
        ledger_block = self.log_to_ledger(model_version)
        
        # Verify ledger integrity
        print(f"\nStep 5: Verifying ledger integrity...")
        verification = self.ledger.verify_chain()
        if verification['valid']:
            print("[SUCCESS] Ledger integrity verified")
        else:
            print("[ERROR] Ledger integrity check failed!")
            print(f"  Errors: {verification.get('errors', [])}")
        
        print(f"\n{'=' * 70}")
        print(f"Training Complete!")
        print(f"{'=' * 70}")
        print(f"\nModel: OneSeek-7B-Zero.v{version}")
        print(f"Ledger Block: {ledger_block['block_id']}")
        print(f"Validation Accuracy: {results['metrics']['validation_accuracy']:.3f}")
        print(f"Fairness Score: {results['metrics']['fairness_score']:.3f}")
        print(f"Demographic Parity: {results['fairness_metrics']['demographic_parity']:.3f}")
        
        # Show if PyTorch was used
        if 'model_used' in results['metrics']:
            print(f"\n[INFO] Training Method: PyTorch with LoRA/PEFT")
            print(f"   Base Model: {results['metrics']['model_used']}")
            print(f"   Device: {results['metrics']['device']}")
            if 'trainable_params' in results['metrics']:
                print(f"   Trainable Parameters: {results['metrics']['trainable_params']:,}")
        else:
            print(f"\n[WARNING]  Training Method: Simulation (PyTorch not available)")
        
        print(f"\nModel saved to: {self.model_dir}")
        print(f"Transparency ledger: {self.ledger.ledger_file}")
        

def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Train OneSeek-7B-Zero Language Model')
    parser.add_argument('--version', type=str, default='1.0.0',
                        help='Model version (semantic versioning, e.g., 1.0.0)')
    parser.add_argument('--data-dir', type=str, default=None,
                        help='Directory containing prepared datasets')
    
    args = parser.parse_args()
    
    # Configuration
    if args.data_dir:
        DATA_DIR = args.data_dir
    else:
        DATA_DIR = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'data', 'prepared'
        )
    
    MODEL_DIR = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        'models', 'oqt-1.0'
    )
    
    LEDGER_DIR = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        'ledger'
    )
    
    # Create trainer and run
    trainer = OneSeekTrainer(DATA_DIR, MODEL_DIR, LEDGER_DIR)
    trainer.train(args.version)


if __name__ == '__main__':
    main()
