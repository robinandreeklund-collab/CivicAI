#!/usr/bin/env python3
"""
OneSeek-7B-Zero Language Model Training
Batch training for the OneSeek-7B-Zero (formerly OQT-1.0) model
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# Add pipelines to path
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), 'pipelines'))

from transparency_ledger import TransparencyLedger


class OneSeekTrainer:
    """Trainer for OneSeek-7B-Zero language model"""
    
    def __init__(self, data_dir: str, model_dir: str, ledger_dir: str):
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize transparency ledger
        self.ledger = TransparencyLedger(ledger_dir)
        
        # Training config
        self.config = {
            'model_name': 'OneSeek-7B-Zero',
            'legacy_name': 'OQT-1.0',  # For backward compatibility
            'architecture': 'transformer',
            'base_models': ['Mistral-7B', 'LLaMA-2'],
            'learning_rate': 2e-5,
            'batch_size': 32,
            'epochs': 3,
            'warmup_steps': 500,
            'use_lora': True,
            'lora_rank': 8,
            'lora_alpha': 32
        }
    
    def load_training_data(self) -> Dict:
        """Load prepared training datasets"""
        datasets = {}
        
        for split in ['train', 'validation', 'test']:
            file_path = self.data_dir / f"{split}.json"
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    datasets[split] = json.load(f)
                print(f"Loaded {len(datasets[split])} {split} samples")
            else:
                print(f"Warning: {split} dataset not found at {file_path}")
                datasets[split] = []
        
        return datasets
    
    def calculate_dataset_hash(self, datasets: Dict) -> str:
        """Calculate hash of all training data for provenance"""
        import hashlib
        combined_data = json.dumps(datasets, sort_keys=True)
        return hashlib.sha256(combined_data.encode()).hexdigest()
    
    def train_model(self, datasets: Dict, version: str) -> Dict:
        """
        Train model - uses PyTorch if available, otherwise simulates
        
        Args:
            datasets: Training datasets
            version: Model version
            
        Returns:
            Training results with metrics
        """
        # Try to import PyTorch training module
        try:
            import sys
            from pathlib import Path
            
            # Add pytorch_trainer to path
            pytorch_trainer_path = Path(__file__).parent
            if str(pytorch_trainer_path) not in sys.path:
                sys.path.insert(0, str(pytorch_trainer_path))
            
            from pytorch_trainer import (
                verify_requirements,
                train_with_pytorch_lora,
                check_base_models
            )
            
            # Check if PyTorch training is available
            if verify_requirements():
                print("\nüéØ PyTorch training environment detected!")
                print("   Using real LoRA/PEFT training...\n")
                
                # Get base models directory
                base_models_dir = self.model_dir.parent / 'base_models'
                
                # Check if base models exist
                available_models = check_base_models(base_models_dir)
                
                if available_models:
                    # Use real PyTorch training
                    return train_with_pytorch_lora(
                        datasets=datasets,
                        version=version,
                        model_dir=self.model_dir,
                        base_models_dir=base_models_dir,
                        config=self.config
                    )
                else:
                    print("\n‚ö†Ô∏è  No base models found. Falling back to simulation.")
                    print(f"   Please download models to: {base_models_dir}")
            else:
                print("\n‚ö†Ô∏è  PyTorch requirements not met. Falling back to simulation.")
                
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Could not use PyTorch training: {e}")
            print("   Falling back to simulation.")
        
        # Fall back to simulation
        return self.simulate_training(datasets, version)
    
    def simulate_training(self, datasets: Dict, version: str) -> Dict:
        """
        Simulate model training
        In production, this would use PyTorch/TensorFlow
        """
        print(f"\n{'=' * 70}")
        print(f"Training OneSeek-7B-Zero Version {version}")
        print(f"{'=' * 70}")
        
        train_size = len(datasets.get('train', []))
        val_size = len(datasets.get('validation', []))
        
        print(f"\nDataset sizes:")
        print(f"  Training: {train_size}")
        print(f"  Validation: {val_size}")
        
        print(f"\nTraining configuration:")
        for key, value in self.config.items():
            print(f"  {key}: {value}")
        
        print(f"\nStarting training...")
        
        # Simulate training metrics
        # In production, these would be actual training results
        metrics = {
            'training_loss': 0.342,
            'validation_accuracy': 0.876,
            'fairness_score': 0.912,
            'bias_score': 0.123,
            'consensus_accuracy': 0.854
        }
        
        fairness_metrics = {
            'demographic_parity': 0.945,
            'equal_opportunity': 0.928,
            'disparate_impact': 0.967
        }
        
        print(f"\nTraining completed!")
        print(f"\nFinal Metrics:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.3f}")
        
        print(f"\nFairness Metrics:")
        for key, value in fairness_metrics.items():
            print(f"  {key}: {value:.3f}")
        
        return {
            'metrics': metrics,
            'fairness_metrics': fairness_metrics
        }
    
    def save_model_version(self, version: str, datasets: Dict, results: Dict) -> Dict:
        """Save model version with complete metadata"""
        
        model_version = {
            'version': version,
            'model_name': 'OneSeek-7B-Zero',
            'legacy_name': 'OQT-1.0',
            'timestamp': datetime.now().isoformat(),
            'base_models': self.config['base_models'],
            'training_config': {
                'dataset_size': len(datasets.get('train', [])),
                'epochs': self.config['epochs'],
                'batch_size': self.config['batch_size'],
                'learning_rate': self.config['learning_rate'],
                'use_lora': self.config['use_lora'],
                'lora_rank': self.config['lora_rank'],
                'lora_alpha': self.config['lora_alpha']
            },
            'metrics': results['metrics'],
            'fairness_metrics': results['fairness_metrics'],
            'provenance': {
                'training_data_hash': self.calculate_dataset_hash(datasets),
                'ledger_block_id': None,  # Will be set after adding to ledger
                'trainer': 'OneSeek-Training-Pipeline',
                'notes': f'Batch training for version {version}'
            }
        }
        
        # Save model metadata file with new naming convention
        # Format: oneseek-7b-zero-v{MAJOR}.{MICRO}.json
        metadata_file = self.model_dir / f"oneseek-7b-zero-v{version}.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(model_version, f, indent=2)
        
        print(f"\nSaved model metadata to {metadata_file}")
        
        # Check if PyTorch weights were saved
        weights_file = self.model_dir / f"oneseek-7b-zero-v{version}.pth"
        if weights_file.exists():
            print(f"‚úÖ Model weights saved to {weights_file}")
        else:
            print(f"‚ÑπÔ∏è  Model weights will be saved to {weights_file} (PyTorch training required)")
        
        # Check if LoRA adapters were saved
        lora_path = self.model_dir.parent / 'lora_adapters' / f'oneseek-7b-zero-v{version}'
        if lora_path.exists():
            print(f"‚úÖ LoRA adapters saved to {lora_path}")
        
        return model_version
    
    def log_to_ledger(self, model_version: Dict) -> Dict:
        """Log training event to transparency ledger"""
        
        ledger_data = {
            'model_version': model_version['version'],
            'training_samples': model_version['training_config']['dataset_size'],
            'fairness_metrics': model_version['fairness_metrics'],
            'bias_metrics': {
                'bias_score': model_version['metrics']['bias_score']
            },
            'provenance': [
                f"training_data_hash:{model_version['provenance']['training_data_hash']}",
                f"trainer:{model_version['provenance']['trainer']}",
                f"timestamp:{model_version['timestamp']}"
            ],
            'training_config': model_version['training_config'],
            'metrics': model_version['metrics']
        }
        
        block = self.ledger.add_block('training', ledger_data, validator='OneSeek-Training-Pipeline')
        
        # Update model version with ledger reference
        model_version['provenance']['ledger_block_id'] = block['block_id']
        
        # Re-save metadata with ledger reference using new naming convention
        version = model_version['version']
        metadata_file = self.model_dir / f"oneseek-7b-zero-v{version}.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(model_version, f, indent=2)
        
        print(f"Logged to transparency ledger (Block {block['block_id']})")
        
        return block
    
    def train(self, version: str = '1.0.0'):
        """
        Main training pipeline
        
        Args:
            version: Semantic version for this model
        """
        print(f"\n{'=' * 70}")
        print(f"OneSeek-7B-Zero Training Pipeline")
        print(f"{'=' * 70}")
        
        # Load data
        print(f"\nStep 1: Loading training data...")
        datasets = self.load_training_data()
        
        if not datasets.get('train'):
            print("ERROR: No training data found!")
            print("Please run prepare_dataset.py first.")
            return
        
        # Train model
        print(f"\nStep 2: Training model...")
        results = self.train_model(datasets, version)
        
        # Save model version
        print(f"\nStep 3: Saving model version...")
        model_version = self.save_model_version(version, datasets, results)
        
        # Log to transparency ledger
        print(f"\nStep 4: Logging to transparency ledger...")
        ledger_block = self.log_to_ledger(model_version)
        
        # Verify ledger integrity
        print(f"\nStep 5: Verifying ledger integrity...")
        verification = self.ledger.verify_chain()
        if verification['valid']:
            print("‚úì Ledger integrity verified")
        else:
            print("‚úó Ledger integrity check failed!")
            print(f"  Errors: {verification.get('errors', [])}")
        
        print(f"\n{'=' * 70}")
        print(f"Training Complete!")
        print(f"{'=' * 70}")
        print(f"\nModel: OneSeek-7B-Zero.v{version}")
        print(f"Ledger Block: {ledger_block['block_id']}")
        print(f"Validation Accuracy: {results['metrics']['validation_accuracy']:.3f}")
        print(f"Fairness Score: {results['metrics']['fairness_score']:.3f}")
        print(f"Demographic Parity: {results['fairness_metrics']['demographic_parity']:.3f}")
        
        # Show if PyTorch was used
        if 'model_used' in results['metrics']:
            print(f"\nüéØ Training Method: PyTorch with LoRA/PEFT")
            print(f"   Base Model: {results['metrics']['model_used']}")
            print(f"   Device: {results['metrics']['device']}")
            if 'trainable_params' in results['metrics']:
                print(f"   Trainable Parameters: {results['metrics']['trainable_params']:,}")
        else:
            print(f"\n‚ö†Ô∏è  Training Method: Simulation (PyTorch not available)")
        
        print(f"\nModel saved to: {self.model_dir}")
        print(f"Transparency ledger: {self.ledger.ledger_file}")
        

def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Train OneSeek-7B-Zero Language Model')
    parser.add_argument('--version', type=str, default='1.0.0',
                        help='Model version (semantic versioning, e.g., 1.0.0)')
    parser.add_argument('--data-dir', type=str, default=None,
                        help='Directory containing prepared datasets')
    
    args = parser.parse_args()
    
    # Configuration
    if args.data_dir:
        DATA_DIR = args.data_dir
    else:
        DATA_DIR = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'data', 'prepared'
        )
    
    MODEL_DIR = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        'models', 'oqt-1.0'
    )
    
    LEDGER_DIR = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        'ledger'
    )
    
    # Create trainer and run
    trainer = OneSeekTrainer(DATA_DIR, MODEL_DIR, LEDGER_DIR)
    trainer.train(args.version)


if __name__ == '__main__':
    main()
