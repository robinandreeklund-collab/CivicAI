# OneSeek-7B-Zero (formerly OQT-1.0) - Complete Documentation

> **Note:** This model was previously known as OQT-1.0 (Open Question-answering Transparent). The new identity **OneSeek-7B-Zero** better reflects its purpose as a transparent, continuously-learning AI agent. Legacy OQT references are maintained for backward compatibility.

## üìã Table of Contents

- [What is OneSeek-7B-Zero?](#what-is-oneseek-7b-zero)
- [Model Identity & Naming](#model-identity--naming)
- [Architecture Overview](#architecture-overview)
- [Complete Data Flow](#complete-data-flow)
- [All Data Points](#all-data-points)
- [Firebase Database Schema](#firebase-database-schema)
- [External AI Services](#external-ai-services)
- [Base Models](#base-models)
- [ML Pipeline Libraries](#ml-pipeline-libraries)
- [BERT Summarizer Integration](#bert-summarizer-integration)
- [Training System](#training-system)
- [Version Management](#version-management)
- [Fine-Tuning & Identity Training](#fine-tuning--identity-training)
- [Ledger & Provenance](#ledger--provenance)
- [OQT Dashboard](#oqt-dashboard)
- [Model Weights Storage](#model-weights-storage)
- [Implementation Status](#implementation-status)
- [API Endpoints](#api-endpoints)
- [Quick Start](#quick-start)
- [Performance Metrics](#performance-metrics)

---

## What is OneSeek-7B-Zero?

**OneSeek-7B-Zero** (formerly OQT-1.0) is a self-contained language model that uses **Mistral 7B** and **LLaMA-2** as base models to create an independent AI system focused on transparency, fairness, and continuous learning.

### Key Characteristics:

- **Independent Language Model**: OneSeek-7B-Zero is its own model, not just a wrapper around external AIs
- **Multi-Model Foundation**: Uses Mistral 7B (fast inference) and LLaMA-2 (deep analysis) as base architectures
- **Continuous Training**: Learns from every interaction through two-step microtraining
- **Transparent**: Every decision, training event, and data source is logged in the ledger
- **Fair & Unbiased**: Active bias detection and fairness metrics in every response
- **Real-time Adaptation**: Updates immediately with new information from external AI sources
- **Identity Training**: Fine-tuned with instruction dataset to embody OpenSeek AI-agent identity

### How It Differs from External AI Services:

| Feature | OneSeek-7B-Zero | External AI (GPT, Gemini, etc.) |
|---------|-----------------|--------------------------------|
| **Purpose** | User interaction, direct queries | Training data collection |
| **Interface** | OQT Dashboard (`/oqt-dashboard`) | Start view (homepage) |
| **Training** | Continuous, real-time | Periodic, provider-controlled |
| **Transparency** | Full ledger, provenance tracking | Black box |
| **Customization** | Adapts to our data & use cases | General purpose |
| **Independence** | Fully self-hosted | Depends on external APIs |
| **Identity** | OpenSeek AI-agent with ethical foundation | Generic assistant |

---

## Model Identity & Naming

### Naming Convention

**Current:** `OneSeek-7B-Zero.v{MAJOR}.{MICRO}`

**Legacy:** `OQT-1.0.v{major}.{micro}` (maintained for backward compatibility)

### Why OneSeek-7B-Zero?

- **OneSeek**: Represents the project for transparent, accountable AI
- **7B**: Indicates 7 billion parameters (Mistral 7B + LLaMA-2 base)
- **Zero**: Marks the starting point for continuous training and evolution

### Version Format

**Format:** `OneSeek-7B-Zero.v{MAJOR}.{MICRO}`

- **MAJOR** (1, 2, 3...): Incremented during weekly/monthly batch training
- **MICRO** (.1, .2, .3...): Incremented during real-time microtraining (two per question)

**Examples:**
```
OneSeek-7B-Zero.v1.0     ‚Üê Initial release after identity training
OneSeek-7B-Zero.v1.1     ‚Üê Microtraining Stage 1 (raw data)
OneSeek-7B-Zero.v1.2     ‚Üê Microtraining Stage 2 (analyzed metrics)
OneSeek-7B-Zero.v1.3     ‚Üê Next question, Stage 1
OneSeek-7B-Zero.v1.4     ‚Üê Next question, Stage 2
...
OneSeek-7B-Zero.v2.0     ‚Üê Next major batch training
```

### Backward Compatibility

Legacy OQT-1.0 references are maintained in:
- API endpoints (e.g., `/api/oqt/query`)
- Firebase collections (e.g., `oqt_queries`, `oqt_training_events`)
- Configuration variables
- Documentation cross-references

New code should use **OneSeek-7B-Zero** naming, but legacy references will continue to work.

---

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CivicAI Platform                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ   Start View     ‚îÇ              ‚îÇ    OQT Dashboard        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ   (Homepage)     ‚îÇ              ‚îÇ  (/oqt-dashboard)       ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ              ‚îÇ                         ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ GPT            ‚îÇ              ‚îÇ ‚Ä¢ Chat with OQT-1.0     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Gemini         ‚îÇ              ‚îÇ ‚Ä¢ Real-time Activity    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Grok           ‚îÇ              ‚îÇ ‚Ä¢ Metrics Tracking      ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Claude         ‚îÇ              ‚îÇ ‚Ä¢ Ledger Transparency   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ DeepSeek       ‚îÇ              ‚îÇ                         ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Qwen           ‚îÇ              ‚îÇ                         ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ           ‚îÇ                                   ‚îÇ                      ‚îÇ
‚îÇ           ‚îÇ Collect Training Data             ‚îÇ User Queries        ‚îÇ
‚îÇ           ‚ñº                                   ‚ñº                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ              Firebase (ai_interactions)                  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Raw responses from external AI                        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ML pipeline analysis (consensus, bias, fairness)      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Quality metrics & provenance tracking                 ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                           ‚îÇ                                          ‚îÇ
‚îÇ                           ‚ñº                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ              OQT-1.0 Training Pipeline                   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Base Models   ‚îÇ        ‚îÇ  ML Service       ‚îÇ        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                ‚îÇ        ‚îÇ  (port 5000)      ‚îÇ        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Mistral 7B   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                   ‚îÇ        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ LLaMA-2      ‚îÇ        ‚îÇ ‚Ä¢ GPU/CPU Auto    ‚îÇ        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ ‚Ä¢ Model Cache     ‚îÇ        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ                            ‚îÇ ‚Ä¢ 8-bit Quant     ‚îÇ        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  Two-Step Training:                                      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  1Ô∏è‚É£ Raw data from external AI ‚Üí Knowledge base          ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  2Ô∏è‚É£ Analyzed metrics ‚Üí Model refinement                 ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                              ‚îÇ                                       ‚îÇ
‚îÇ                              ‚ñº                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ         OQT-1.0 Model Weights (Versioned)               ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  models/oqt/weights/oqt-1.0-v{major}.{micro}.pth        ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Complete Data Flow

### 11-Step Process: From User Question to Trained Model

```
1. USER QUESTION
   ‚Üì
   User submits question via Start View or OQT Dashboard

2. EXTERNAL AI QUERIES (Start View Only)
   ‚Üì
   ‚Ä¢ GPT-4, Gemini, Grok, Claude, DeepSeek, Qwen
   ‚Ä¢ 6 parallel requests to external AI services

3. RAW RESPONSE STORAGE
   ‚Üì
   Firebase: ai_interactions.raw_responses[]
   ‚Ä¢ Service name, response text, timestamp, latency

4. ML PIPELINE ANALYSIS
   ‚Üì
   Multi-model pipeline processes all responses:
   ‚Ä¢ Sentiment analysis
   ‚Ä¢ Tone detection
   ‚Ä¢ Bias measurement
   ‚Ä¢ Perspective diversity

5. CONSENSUS/BIAS/FAIRNESS CALCULATION
   ‚Üì
   Firebase: ai_interactions.processed_data{}
   ‚Ä¢ Consensus score (0-1): Agreement between models
   ‚Ä¢ Bias score (0-10): Tonal/perspective bias
   ‚Ä¢ Fairness index (0-1): Inclusivity across perspectives

6. META-SUMMARY GENERATION
   ‚Üì
   Synthesizes insights across all AI responses
   ‚Ä¢ Key themes, agreements, disagreements
   ‚Ä¢ Recommendations for OQT-1.0 response

7. STAGE 1 MICROTRAINING: RAW DATA
   ‚Üì
   OQT-1.0 trains on raw AI responses
   ‚Ä¢ Updates knowledge base
   ‚Ä¢ Learns language patterns
   ‚Ä¢ Improves response generation
   Firebase: oqt_training_events (stage: "raw_data")

8. STAGE 2 MICROTRAINING: ANALYZED DATA
   ‚Üì
   OQT-1.0 trains on pipeline analysis results
   ‚Ä¢ Updates fairness awareness
   ‚Ä¢ Refines bias detection
   ‚Ä¢ Improves consensus understanding
   Firebase: oqt_training_events (stage: "analyzed_data")

9. MODEL WEIGHTS UPDATE
   ‚Üì
   New version created: OQT-1.0.v{major}.{micro}
   ‚Ä¢ Weights saved to models/oqt/weights/
   ‚Ä¢ Metadata logged to Firebase
   ‚Ä¢ Previous version archived

10. LEDGER BLOCK CREATION
    ‚Üì
    Firebase: oqt_ledger
    ‚Ä¢ Immutable record: Question ‚Üí Responses ‚Üí Analysis ‚Üí Training ‚Üí Version
    ‚Ä¢ Timestamp, hash, provenance chain
    ‚Ä¢ Full transparency

11. OQT-1.0 RESPONSE DELIVERY
    ‚Üì
    Dashboard displays:
    ‚Ä¢ OQT-1.0's synthesized answer
    ‚Ä¢ Confidence score
    ‚Ä¢ Provenance information
    ‚Ä¢ Model version used
```

---

## All Data Points

Every piece of information captured in the OQT-1.0 system:

### Input Data
- **User Question**
  - Question text
  - Timestamp
  - User ID (if authenticated)
  - Source (Start View vs OQT Dashboard)

### External AI Responses (from Start View)
- **Per Service** (GPT, Gemini, Grok, Claude, DeepSeek, Qwen):
  - Service name
  - Response text
  - Response time (ms)
  - Timestamp
  - Token count
  - Model version

### ML Pipeline Analysis
- **Per Response**:
  - Sentiment (positive/negative/neutral, score 0-1)
  - Tone (formal/casual/technical, confidence %)
  - Bias indicators (political, cultural, demographic)
  - Perspective coverage (viewpoints represented)
  - Quality score (coherence, relevance)

### Aggregated Metrics
- **Consensus**:
  - Sentiment agreement (%)
  - Tone agreement (%)
  - Bias variance (0-10)
  - Overall consensus score (0-1)
  - Consensus level (high/medium/low)

- **Bias**:
  - Per-model bias scores
  - Aggregated bias score (0-10)
  - Bias types detected (list)
  - Bias level (low/medium/high)

- **Fairness**:
  - Perspective diversity (0-1)
  - Coverage completeness (%)
  - Fairness index (0-1)
  - Fairness level (excellent/good/fair/poor)

### Meta-Summary
- Summary text
- Key themes (list)
- Agreement points (list)
- Disagreement points (list)
- Recommendations for OQT-1.0

### Training Data
- **Stage 1** (Raw Data):
  - Training samples processed (count)
  - Knowledge base updates
  - Batch size
  - Learning rate
  - Loss metrics

- **Stage 2** (Analyzed Data):
  - Metrics updated (list)
  - Model adjustments made
  - Bias correction applied
  - Fairness improvements

### Model Versioning
- Version number (major.micro)
- Previous version
- Training timestamp
- Samples processed (total)
- Performance delta (vs previous)

### Provenance & Ledger
- Question ID
- Processing steps (list with timestamps)
- Data sources (external AIs used)
- Pipeline version
- Model version used
- Ledger block hash
- Parent block reference

### OQT-1.0 Response
- Response text
- Confidence score (0-1)
- Base models used (Mistral 7B, LLaMA-2)
- Model version
- Generation time (ms)
- Provenance reference

---

## Firebase Database Schema

### 1. `ai_interactions` (from PR #44)

**Purpose**: Stores all data from external AI queries and pipeline analysis

**Structure**:
```javascript
{
  id: "auto-generated-id",
  question: {
    text: "User's question",
    timestamp: "2025-11-20T12:00:00Z",
    user_id: "user-123",
    source: "start_view" | "oqt_dashboard"
  },
  
  raw_responses: [
    {
      service: "gpt4" | "gemini" | "grok" | "claude" | "deepseek" | "qwen",
      response: "AI's raw response text",
      timestamp: "2025-11-20T12:00:01Z",
      latency_ms: 1234,
      tokens: 150,
      model_version: "gpt-4-turbo"
    }
    // ... more services
  ],
  
  processed_data: {
    per_response_analysis: [
      {
        service: "gpt4",
        sentiment: { label: "positive", score: 0.85 },
        tone: { type: "formal", confidence: 0.9 },
        bias_indicators: ["political_left"],
        perspective: ["western", "academic"],
        quality_score: 0.88
      }
      // ... more analyses
    ],
    
    consensus: {
      sentiment_agreement: 0.92,
      tone_agreement: 0.87,
      bias_variance: 2.3,
      score: 0.95,
      level: "high",
      agreements: ["Democracy is important", "..."],
      disagreements: ["Implementation details"]
    },
    
    bias: {
      per_model_scores: [3.2, 2.8, 4.1, 2.9, 3.5, 3.0],
      aggregated_score: 3.25,
      types: ["political", "cultural"],
      level: "low"
    },
    
    fairness: {
      perspective_diversity: 0.88,
      coverage: 0.82,
      score: 0.85,
      level: "excellent"
    },
    
    meta_summary: {
      text: "All models agree that...",
      key_themes: ["democracy", "participation"],
      recommendations: "OQT-1.0 should emphasize..."
    }
  },
  
  pipeline_metadata: {
    version: "1.2.0",
    processing_time_ms: 5432,
    steps_completed: ["raw_collection", "analysis", "aggregation"],
    status: "completed" | "processing" | "failed"
  },
  
  ledger_blocks: [
    "ledger-block-hash-1",
    "ledger-block-hash-2"
  ],
  
  created_at: "2025-11-20T12:00:00Z",
  updated_at: "2025-11-20T12:00:06Z"
}
```

### 2. `oqt_queries`

**Purpose**: Stores queries made directly to OQT-1.0 via the dashboard

**Structure**:
```javascript
{
  id: "auto-generated-id",
  question: "User's question to OQT-1.0",
  timestamp: "2025-11-20T12:00:00Z",
  user_id: "user-123",
  
  oqt_response: {
    text: "OQT-1.0's synthesized answer",
    confidence: 0.92,
    base_models: ["mistral-7b", "llama-2-7b"],
    model_version: "OQT-1.0.v13.2",
    generation_time_ms: 856
  },
  
  provenance: {
    question_id: "ai_interactions/doc-id",
    training_data_sources: ["gpt4", "gemini", "grok"],
    pipeline_version: "1.2.0",
    ledger_block: "ledger-block-hash"
  },
  
  feedback: {
    rating: 4,
    helpful: true,
    comment: "Very clear explanation"
  }
}
```

### 3. `oqt_training_events`

**Purpose**: Logs every training session (both large dataset and microtraining)

**Structure**:
```javascript
{
  id: "auto-generated-id",
  event_type: "major_training" | "microtraining",
  timestamp: "2025-11-20T12:00:00Z",
  
  version: {
    previous: "OQT-1.0.v13.1",
    new: "OQT-1.0.v13.2",
    type: "major" | "micro"
  },
  
  training_data: {
    stage: "raw_data" | "analyzed_data",
    samples_processed: 1,
    batch_size: 1,
    source_interaction: "ai_interactions/doc-id"
  },
  
  model_config: {
    base_models: ["mistral-7b", "llama-2-7b"],
    learning_rate: 0.0001,
    epochs: 1,
    optimizer: "adamw"
  },
  
  performance: {
    loss_before: 0.245,
    loss_after: 0.241,
    improvement: 0.004,
    metrics_updated: ["fairness", "bias_detection"]
  },
  
  weights: {
    path: "models/oqt/weights/oqt-1.0-v13.2.pth",
    size_mb: 13420,
    checksum: "sha256-hash"
  },
  
  ledger_block: "ledger-block-hash",
  duration_seconds: 45
}
```

### 4. `oqt_metrics`

**Purpose**: Tracks model performance over time

**Structure**:
```javascript
{
  id: "auto-generated-id",
  model_version: "OQT-1.0.v13.2",
  timestamp: "2025-11-20T12:00:00Z",
  
  performance: {
    average_confidence: 0.89,
    response_time_ms: 850,
    accuracy: 0.91, // if ground truth available
    user_satisfaction: 4.2 // average rating
  },
  
  fairness_metrics: {
    bias_score: 2.1,
    fairness_index: 0.88,
    perspective_diversity: 0.85
  },
  
  training_stats: {
    total_samples: 15234,
    last_major_training: "2025-11-13T00:00:00Z",
    microtraining_events: 1523,
    training_frequency: "real-time"
  },
  
  usage: {
    queries_processed: 1523,
    queries_today: 45,
    active_users: 123
  }
}
```

### 5. `oqt_ledger`

**Purpose**: Immutable blockchain-style ledger for transparency

**Structure**:
```javascript
{
  id: "ledger-block-hash",
  block_number: 1523,
  timestamp: "2025-11-20T12:00:00Z",
  
  transaction_type: "training" | "query" | "model_update",
  
  data: {
    question: "User's question",
    ai_services_used: ["gpt4", "gemini", "grok"],
    training_stages: ["raw_data", "analyzed_data"],
    model_version: "OQT-1.0.v13.2",
    metrics: {
      consensus: 0.95,
      bias: 2.1,
      fairness: 0.88
    }
  },
  
  provenance: {
    source_interaction: "ai_interactions/doc-id",
    training_event: "oqt_training_events/doc-id",
    previous_block: "ledger-block-hash-prev"
  },
  
  hash: "sha256-current-block-hash",
  previous_hash: "sha256-previous-block-hash",
  
  immutable: true
}
```

---

## External AI Services

OQT-1.0 learns from 6 external AI services (via Start View):

| Service | Purpose | Response Time | Usage |
|---------|---------|---------------|-------|
| **GPT-4** | General knowledge, reasoning | ~2s | Training data |
| **Gemini** | Factual accuracy, multi-modal | ~1.5s | Training data |
| **Grok** | Real-time info, conversational | ~1s | Training data |
| **Claude** | Detailed analysis, ethical reasoning | ~2.5s | Training data |
| **DeepSeek** | Technical depth, coding | ~2s | Training data |
| **Qwen** | Multilingual, cultural diversity | ~1.8s | Training data |

**Key Points**:
- External AIs are **NOT** used for direct user interaction
- They provide **training data** for OQT-1.0
- All responses are analyzed by ML pipeline before training
- Users interact **only with OQT-1.0** via the dashboard

---

## Base Models

OQT-1.0 is built on two foundational models:

### Mistral 7B
- **Purpose**: Fast real-time inference
- **Size**: 7 billion parameters
- **Speed**: ~100ms per response
- **Strength**: Quick responses, conversational
- **Source**: `mistralai/Mistral-7B-Instruct`

### LLaMA-2 (7B/13B)
- **Purpose**: Deep linguistic analysis
- **Size**: 7-13 billion parameters
- **Speed**: ~300ms per response
- **Strength**: Comprehensive understanding, nuanced reasoning
- **Source**: `meta-llama/Llama-2-7b-chat-hf` or `Llama-2-13b-chat-hf`

### How They Work Together:

```
User Question
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Mistral 7B    ‚îÇ ‚Üí Fast initial response
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLaMA-2       ‚îÇ ‚Üí Deep analysis & refinement
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OQT-1.0       ‚îÇ ‚Üí Synthesized, optimized response
‚îÇ  (Our Model)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ML Pipeline Libraries

The OQT-1.0 ML pipeline uses a comprehensive suite of specialized libraries for analysis, processing, and transparency. All results are stored in Firebase (`ai_interactions` collection) after analysis.

### Libraries & Their Functions

#### 1. **spaCy** - NLP Core Processing
- **Purpose**: Natural Language Processing foundation
- **Functions**:
  - Tokenization (breaking text into words/sentences)
  - Part-of-Speech (POS) tagging
  - Named Entity Recognition (NER)
  - Dependency parsing
  - Lemmatization
- **Use in OQT**: Base text processing for all AI responses, extracting key entities and grammatical structures
- **Storage**: `ai_interactions.processed_data.nlp_features`

#### 2. **TextBlob** - Sentiment Analysis
- **Purpose**: Simple but effective sentiment and language analysis
- **Functions**:
  - Sentiment polarity (-1 to +1)
  - Subjectivity detection (0 to 1)
  - Basic translation
  - Noun phrase extraction
- **Use in OQT**: Quick sentiment scoring of AI responses to detect emotional tone
- **Storage**: `ai_interactions.processed_data.sentiment`

#### 3. **langdetect** - Language Identification
- **Purpose**: Automatic language detection
- **Functions**:
  - Detects language of input text
  - Supports 55+ languages
  - Returns ISO language codes
- **Use in OQT**: Ensures responses are in expected language (Swedish/English), flags mixed-language responses
- **Storage**: `ai_interactions.processed_data.language`

#### 4. **Detoxify** - Toxicity Detection
- **Purpose**: Identify harmful, toxic, or offensive content
- **Functions**:
  - Toxicity score (0-1)
  - Severe toxicity detection
  - Obscenity, threats, insults detection
  - Identity-based hate detection
- **Use in OQT**: Safety filter for AI responses, bias detection component
- **Storage**: `ai_interactions.quality_metrics.toxicity`

#### 5. **Transformers (HuggingFace)** - Modern LLM Framework
- **Purpose**: Access to state-of-the-art language models
- **Functions**:
  - Load and run Mistral 7B, LLaMA-2
  - LoRA/PEFT fine-tuning
  - Model inference and generation
  - Tokenization for all models
- **Use in OQT**: Core framework for running base models and LoRA adapters
- **Storage**: Model weights in `models/oqt/weights/`

#### 6. **SHAP** - Explainability (Feature Importance)
- **Purpose**: Explain model predictions
- **Functions**:
  - Feature importance calculation
  - Shapley values for each input
  - Visual explanation plots
  - Model-agnostic explanations
- **Use in OQT**: Transparency layer - shows which parts of input influenced the response
- **Storage**: `ai_interactions.pipeline_metadata.explainability.shap_values`

#### 7. **Gensim** - Topic Modeling & Word Embeddings
- **Purpose**: Discover topics and semantic relationships
- **Functions**:
  - Word2Vec embeddings
  - Topic modeling (LDA)
  - Document similarity
  - Text summarization
- **Use in OQT**: Identify common themes across AI responses, semantic clustering
- **Storage**: `ai_interactions.processed_data.topics`

#### 8. **BERTopic** - Advanced Topic Modeling
- **Purpose**: State-of-the-art topic discovery
- **Functions**:
  - Dynamic topic modeling
  - BERT-based embeddings
  - Hierarchical topics
  - Topic evolution over time
- **Use in OQT**: Deep topic analysis for consensus detection, trend tracking in dashboard
- **Storage**: `ai_interactions.processed_data.bert_topics`

#### 9. **LIME** - Local Model Explainability
- **Purpose**: Explain individual predictions locally
- **Functions**:
  - Local approximations of model behavior
  - Feature importance per prediction
  - Human-interpretable explanations
  - Works with any model
- **Use in OQT**: Per-response explanation of why OQT gave specific answer
- **Storage**: `ai_interactions.pipeline_metadata.explainability.lime_explanation`

#### 10. **Fairlearn** - Fairness Analysis
- **Purpose**: Detect and mitigate bias in AI models
- **Functions**:
  - Fairness metrics calculation
  - Disparity measurement
  - Bias mitigation algorithms
  - Group fairness evaluation
- **Use in OQT**: Core fairness scoring component, ensures balanced responses across perspectives
- **Storage**: `ai_interactions.quality_metrics.fairness`

#### 11. **Lux** - Data Exploration
- **Purpose**: Automated data visualization and exploration
- **Functions**:
  - Auto-generate visualizations
  - Pattern discovery
  - Data quality insights
  - Interactive exploration
- **Use in OQT**: Dashboard data exploration in "M√§tv√§rden" tab, quick insights into model behavior
- **Storage**: Used for visualization, not stored in Firebase

#### 12. **Sweetviz** - Data Insights & Visualization
- **Purpose**: Comprehensive data profiling and comparison
- **Functions**:
  - Automated EDA (Exploratory Data Analysis)
  - Dataset comparison
  - Feature correlation
  - Distribution analysis
- **Use in OQT**: Model performance analysis, comparing training datasets
- **Storage**: Reports generated for dashboard, metrics in `oqt_metrics`

### Pipeline Flow with Libraries

```
User Question
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  External AI Responses (6 services)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ML Pipeline Analysis                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ spaCy: Tokenize, POS, NER      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ langdetect: Language check     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ TextBlob: Sentiment scoring    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Detoxify: Toxicity detection   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Gensim: Topic extraction       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ BERTopic: Deep topic modeling  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Fairlearn: Fairness analysis   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ SHAP: Feature importance       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ LIME: Local explainability     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
Save to ai_interactions.processed_data
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BERT Summarizer (see next section) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
Two-Step Training ‚Üí OQT-1.0 Response
```

### Implementation Status

| Library | Status | Use Case |
|---------|--------|----------|
| spaCy | ‚úÖ Integrated | Base NLP processing |
| TextBlob | ‚úÖ Integrated | Sentiment analysis |
| langdetect | ‚úÖ Integrated | Language detection |
| Detoxify | ‚úÖ Integrated | Toxicity scoring |
| Transformers | ‚úÖ Integrated | Model inference |
| SHAP | üîÑ Partial | Explainability (in progress) |
| Gensim | ‚úÖ Integrated | Topic modeling |
| BERTopic | ‚úÖ Integrated | Advanced topics, hierarchical modeling |
| LIME | ‚úÖ Integrated | Local explanations per prediction |
| Fairlearn | ‚úÖ Integrated | Fairness metrics |
| Lux | ‚úÖ Integrated | Automated data visualization |
| Sweetviz | ‚úÖ Integrated | Data profiling and EDA |

---

## BERT Summarizer Integration

To enhance OQT-1.0's ability to provide user-friendly and transparent responses, **BERT-Summarizer** is used as a summarization layer in the ML pipeline. The summarizer generates concise, balanced overviews of both raw responses and analyses, enabling OQT to present complex information clearly.

### Purpose & Benefits

1. **Transparency**: Users get both detailed responses and clear summaries
2. **Efficiency**: OQT can quickly summarize complex multi-model debates
3. **Identity Reinforcement**: Strengthens OQT's role as an "OpenSeek AI-agent" for clarity and transparency
4. **User Experience**: Reduces cognitive load with concise overviews before diving into details

### Flow Integration

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Raw AI Responses                ‚îÇ
‚îÇ  ‚Ä¢ ChatGPT, Gemini, Grok, etc.      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. BERT Summarizer                 ‚îÇ
‚îÇ  ‚Ä¢ Compresses 6 responses           ‚îÇ
‚îÇ  ‚Ä¢ Extracts key points              ‚îÇ
‚îÇ  ‚Ä¢ Creates balanced overview        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. Analysis Results                ‚îÇ
‚îÇ  ‚Ä¢ Consensus, Bias, Fairness        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. BERT Summarizer (metaSummary)   ‚îÇ
‚îÇ  ‚Ä¢ Compresses analysis results      ‚îÇ
‚îÇ  ‚Ä¢ Creates human-readable summary   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. Training Data                   ‚îÇ
‚îÇ  ‚Ä¢ Summarized responses used        ‚îÇ
‚îÇ  ‚Ä¢ Reinforces OQT identity          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. Dashboard Display               ‚îÇ
‚îÇ  ‚Ä¢ Shows both full text & summary   ‚îÇ
‚îÇ  ‚Ä¢ Includes provenance              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Example Usage

**Input**: Raw responses from 3 AI services on EU climate policy

```yaml
summarizer:
  input:
    - "ChatGPT: EU b√∂r fokusera p√• gemensamma utsl√§ppsm√•l."
    - "Gemini: Nationella l√∂sningar √§r mer effektiva."
    - "Grok: Teknologisk innovation b√∂r prioriteras."
  
  output: "Modellerna √§r √∂verens om klimatm√•l, men skiljer sig i synen p√• nationell flexibilitet."
  
  metadata:
    oqt_version: "OQT-1.0.v12.6"
    ledger_timestamp: "2025-11-20T22:25:00Z"
    compression_ratio: 0.35
    key_themes: ["klimatm√•l", "nationell flexibilitet", "innovation"]
```

### Storage in Firebase

Summaries are stored in `ai_interactions`:

```json
{
  "question_id": "q_2025_11_20_001",
  "raw_responses": [ ... ],
  "processed_data": {
    "raw_summary": {
      "text": "Modellerna √§r √∂verens om klimatm√•l...",
      "compression_ratio": 0.35,
      "key_themes": ["klimatm√•l", "nationell flexibilitet"],
      "generated_at": "2025-11-20T22:25:00Z"
    },
    "meta_summary": {
      "text": "Konsensus: H√∂g (0.87). Bias: L√•g (0.12). Fairness: Utm√§rkt (0.91).",
      "analysis_compressed": true,
      "provenance": "#interaction_2025_11_20_001"
    }
  }
}
```

### Dashboard Presentation

In the OQT Dashboard, summaries are displayed prominently:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ü§ñ OpenSeek AI-agent                    ‚îÇ
‚îÇ  OQT-1.0.v12.6                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Sammanfattning:                         ‚îÇ
‚îÇ  "Modellerna √§r √∂verens om klimatm√•l,    ‚îÇ
‚îÇ   men skiljer sig i synen p√• nationell   ‚îÇ
‚îÇ   flexibilitet."                         ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  [Visa fulltext] [Visa analys]          ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  Fairness: 0.87 | Bias: L√•g             ‚îÇ
‚îÇ  Provenance: #interaction_2025_11_20_001‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Training Enhancement

Summaries improve OQT training by:

1. **Clearer Signal**: Condensed information is easier to learn from
2. **Identity Reinforcement**: OQT learns to respond with "OpenSeek clarity"
3. **Faster Convergence**: Less noise in training data
4. **Better Generalization**: Focuses on core concepts, not verbosity

### Implementation Status

| Component | Status | Notes |
|-----------|--------|-------|
| BERT Summarizer Library | üîÑ Integration | Using `bert-extractive-summarizer` |
| Raw Response Summarization | üìã Planned | Week 1-2 |
| Analysis Summarization (metaSummary) | üìã Planned | Week 2-3 |
| Dashboard Display | ‚úÖ UI Ready | Awaits summarizer output |
| Training Integration | üìã Planned | Week 3-4 |
| Provenance Tracking | ‚úÖ Complete | Ledger integration ready |

### Configuration

```python
# BERT Summarizer Config
summarizer_config = {
    "model": "bert-base-multilingual-cased",  # Supports Swedish
    "max_length": 150,  # Target summary length
    "min_length": 50,
    "ratio": 0.3,  # 30% of original length
    "use_first": False,  # Don't always use first sentence
    "algorithm": "extractive"  # Extract key sentences
}
```

---

## Training System

OQT-1.0 uses a **dual training approach**: large dataset training + real-time microtraining.

### 1. Large Dataset Training (Major Versions)

**Frequency**: Weekly or Monthly  
**Version Format**: `OQT-1.0.v13` (major version bump)  
**Data Source**: Accumulated data from `ai_interactions`

**Process**:
```
1. Collect data from past week/month
   ‚Üì
2. Aggregate all raw responses (6 AI services √ó N questions)
   ‚Üì
3. Include all pipeline analysis results
   ‚Üì
4. Full retraining of OQT-1.0 model
   ‚Üì
5. Comprehensive validation & testing
   ‚Üì
6. Deploy new major version: OQT-1.0.v14
   ‚Üì
7. Log to oqt_training_events & oqt_ledger
```

**Characteristics**:
- Large batch size (thousands of samples)
- Multiple epochs
- Full model fine-tuning
- Extensive validation
- Creates checkpoint for rollback

### 2. Real-time Microtraining (Micro Versions)

**Frequency**: On every new question  
**Version Format**: `OQT-1.0.v13.2` (micro version increment)  
**Data Source**: Single question from `ai_interactions`

**Two-Step Process**:

#### Step 1: Raw Data Training
```
Triggered: When new question added to ai_interactions
Data: raw_responses[] from all 6 AI services
Updates: Knowledge base, language patterns
Duration: ~30-60 seconds
Result: OQT-1.0.v13.2 (micro increment)
Logged: oqt_training_events (stage: "raw_data")
```

#### Step 2: Analyzed Data Training
```
Triggered: After ML pipeline completes
Data: processed_data{} (consensus, bias, fairness)
Updates: Fairness awareness, bias detection, meta-understanding
Duration: ~30-60 seconds
Result: OQT-1.0.v13.3 (another micro increment)
Logged: oqt_training_events (stage: "analyzed_data")
```

**Characteristics**:
- Small batch size (1 question, 6 responses)
- Single epoch
- Targeted fine-tuning
- Fast execution
- Incremental improvement

### Training Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    New Question                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  External AI Responses (6 services)                      ‚îÇ
‚îÇ  Saved to: ai_interactions.raw_responses[]               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîÑ STAGE 1 MICROTRAINING                                ‚îÇ
‚îÇ  ‚Ä¢ Input: Raw AI responses                               ‚îÇ
‚îÇ  ‚Ä¢ Model: OneSeek-7B-Zero.v13.1                          ‚îÇ
‚îÇ  ‚Ä¢ Process: Learn language patterns                      ‚îÇ
‚îÇ  ‚Ä¢ Output: OneSeek-7B-Zero.v13.2                         ‚îÇ
‚îÇ  ‚Ä¢ Time: ~45 seconds                                     ‚îÇ
‚îÇ  ‚Ä¢ Log: oqt_training_events                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ML Pipeline Analysis                                    ‚îÇ
‚îÇ  Saved to: ai_interactions.processed_data{}              ‚îÇ
‚îÇ  ‚Ä¢ Consensus, Bias, Fairness calculated                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîÑ STAGE 2 MICROTRAINING                                ‚îÇ
‚îÇ  ‚Ä¢ Input: Analyzed metrics                               ‚îÇ
‚îÇ  ‚Ä¢ Model: OneSeek-7B-Zero.v13.2                          ‚îÇ
‚îÇ  ‚Ä¢ Process: Refine fairness & bias detection             ‚îÇ
‚îÇ  ‚Ä¢ Output: OneSeek-7B-Zero.v13.3                         ‚îÇ
‚îÇ  ‚Ä¢ Time: ~45 seconds                                     ‚îÇ
‚îÇ  ‚Ä¢ Log: oqt_training_events                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Model Weights Saved                                     ‚îÇ
‚îÇ  ‚Ä¢ Path: models/oneseek-7b-zero/weights/v13.3.pth        ‚îÇ
‚îÇ  ‚Ä¢ Metadata: JSON with training info                     ‚îÇ
‚îÇ  ‚Ä¢ Backup: Firebase Storage                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ledger Block Created                                    ‚îÇ
‚îÇ  ‚Ä¢ Full provenance chain                                 ‚îÇ
‚îÇ  ‚Ä¢ Immutable record                                      ‚îÇ
‚îÇ  ‚Ä¢ Saved to: oqt_ledger                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Version Management

OQT-1.0 uses semantic versioning with major and micro versions:

### Format: `OQT-1.0.v{MAJOR}.{MICRO}`

### Major Versions (v13, v14, v15...)

**Created When**: Large dataset training (weekly/monthly)  
**Example**: `OQT-1.0.v13`  
**Increment Rule**: Major version bumps after full retraining  

**Characteristics**:
- Significant model improvements
- Trained on thousands of samples
- Comprehensive testing before deployment
- Checkpoint saved for rollback
- Major performance gains

### Micro Versions (.1, .2, .3...)

**Created When**: Real-time microtraining (every question)  
**Example**: `OQT-1.0.v13.2`  
**Increment Rule**: Micro version increments after each training stage  

**Characteristics**:
- Incremental improvements
- Fast updates
- Continuous learning
- Two increments per question (Stage 1 + Stage 2)

### Example Version History:

```
OQT-1.0.v13.0    ‚Üê Major training (weekly batch)
OQT-1.0.v13.1    ‚Üê Microtraining (Stage 1: raw data)
OQT-1.0.v13.2    ‚Üê Microtraining (Stage 2: analyzed data)
OQT-1.0.v13.3    ‚Üê Microtraining (Stage 1: raw data)
OQT-1.0.v13.4    ‚Üê Microtraining (Stage 2: analyzed data)
...
OQT-1.0.v13.156  ‚Üê After 78 questions (78 √ó 2 stages)
OQT-1.0.v14.0    ‚Üê Next major training (weekly batch)
```

### Version Tracking

All versions logged in:
- **`oqt_training_events`**: Training details, performance metrics
- **`oqt_metrics`**: Performance tracking over time
- **`oqt_ledger`**: Immutable version history

---

## Fine-Tuning & Identity Training

OQT-1.0 uses **LoRA (Low-Rank Adaptation) / PEFT (Parameter-Efficient Fine-Tuning)** for efficient real-time updates while maintaining the base model architecture.

### LoRA/PEFT Implementation

**Why LoRA/PEFT?**
- Fast updates without full model retraining
- Memory-efficient (only trains small adapter layers)
- Preserves base model quality
- Enables real-time microtraining
- Easy version management (swap adapter weights)

**Technical Details**:
```python
# LoRA Configuration
lora_config = {
    "r": 8,                    # Rank of adaptation matrices
    "lora_alpha": 32,          # Scaling factor
    "target_modules": ["q_proj", "v_proj"],  # Which layers to adapt
    "lora_dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM"
}

# Applied to both base models
mistral_7b_lora = apply_lora(mistral_7b, lora_config)
llama2_lora = apply_lora(llama_2, lora_config)
```

**Storage Structure**:
```
models/oqt/weights/
‚îú‚îÄ‚îÄ base_models/
‚îÇ   ‚îú‚îÄ‚îÄ mistral-7b/           # Base model (unchanged)
‚îÇ   ‚îî‚îÄ‚îÄ llama-2-7b/           # Base model (unchanged)
‚îú‚îÄ‚îÄ lora_adapters/
‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v13.0/        # Major version LoRA weights
‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v13.1/        # Micro version LoRA weights
‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v13.2/        # Micro version LoRA weights
‚îÇ   ‚îî‚îÄ‚îÄ current -> oqt-1.0-v13.2  # Symlink to active version
‚îî‚îÄ‚îÄ checkpoints/
    ‚îî‚îÄ‚îÄ daily/
```

### Real-Time Fine-Tuning Flow

**On Every New Question**:

```
1. Question arrives in ai_interactions
   ‚Üì
2. External AI responses collected (raw_responses[])
   ‚Üì
3. üîÑ STAGE 1 MICROTRAINING (LoRA)
   ‚Ä¢ Load current LoRA adapter (e.g., OQT-1.0.v13.1)
   ‚Ä¢ Fine-tune on raw AI responses
   ‚Ä¢ Save new LoRA adapter (OQT-1.0.v13.2)
   ‚Ä¢ Duration: ~30-60 seconds
   ‚Ä¢ Creates micro version: OQT-1.0.v13.2
   ‚Üì
4. ML Pipeline analyzes responses (processed_data{})
   ‚Üì
5. üîÑ STAGE 2 MICROTRAINING (LoRA)
   ‚Ä¢ Load current LoRA adapter (OQT-1.0.v13.2)
   ‚Ä¢ Fine-tune on analyzed metrics
   ‚Ä¢ Save new LoRA adapter (OQT-1.0.v13.3)
   ‚Ä¢ Duration: ~30-60 seconds
   ‚Ä¢ Creates micro version: OQT-1.0.v13.3
   ‚Üì
6. Model ready for next query
```

### Instruction Dataset & OpenSeek Identity

OQT-1.0 is trained to have a distinct **OpenSeek identity** rather than generic AI responses.

#### 1. Instruction Dataset Creation

**Purpose**: Give OQT-1.0 a unique voice and identity as "OpenSeek AI-agent"

**Dataset Size**: 100-500 carefully crafted examples  
**Format**: Instruction ‚Üí Input ‚Üí Output

**Example Instructions**:

```json
[
  {
    "instruction": "Vem √§r du?",
    "input": "",
    "output": "Jag √§r OpenSeek AI-agent, skapad f√∂r transparens och r√§ttvisa analyser. Jag anv√§nder Mistral 7B och LLaMA-2 som basmodeller och tr√§nas kontinuerligt f√∂r att ge balanserade, transparenta svar."
  },
  {
    "instruction": "Vad g√∂r du?",
    "input": "",
    "output": "Jag analyserar svar fr√•n flera AI-modeller (GPT, Gemini, Grok, Claude, DeepSeek, Qwen), ber√§knar konsensus och bias, och ger transparenta, balanserade svar baserat p√• denna analys. Alla mina beslut loggas i en transparent ledger."
  },
  {
    "instruction": "Vilken version √§r du?",
    "input": "",
    "output": "Jag √§r OQT-1.0.v{version}, en sj√§lvst√§ndig spr√•kmodell utvecklad av OpenSeek-teamet. Jag tr√§nas kontinuerligt - b√•de genom stora dataset veckovis och mikrotr√§ning vid varje ny fr√•ga."
  },
  {
    "instruction": "Hur skiljer du dig fr√•n andra AI?",
    "input": "",
    "output": "Till skillnad fr√•n externa AI-tj√§nster √§r jag helt transparent. Du kan se exakt vilken data jag tr√§nats p√•, hur jag fattar beslut, och sp√•ra min utveckling genom ledger-systemet. Jag kombinerar ocks√• insikter fr√•n flera AI-modeller ist√§llet f√∂r att ge en enskild modells perspektiv."
  },
  {
    "instruction": "Vad √§r din specialitet?",
    "input": "",
    "output": "Min specialitet √§r transparent beslutsfattande och r√§ttvisa analyser. Jag ber√§knar konsensus mellan olika AI-perspektiv, identifierar bias, och s√§kerst√§ller att svaren t√§cker olika synvinklar. Allt sp√•ras i min ledger f√∂r full transparens."
  },
  {
    "instruction": "Hur tr√§nas du?",
    "input": "",
    "output": "Jag tr√§nas i tv√• steg: F√∂rst p√• r√•data fr√•n externa AI-tj√§nster, sedan p√• analyserade metriker (konsensus, bias, r√§ttvisa). Detta sker b√•de veckovis (stora dataset) och i realtid vid varje ny fr√•ga (mikrotr√§ning). Varje tr√§ningsh√§ndelse loggas f√∂r transparens."
  }
]
```

#### 2. Initial Fine-Tuning (One-Time Setup)

**Process**:
```bash
# 1. Create instruction dataset
python scripts/create_instruction_dataset.py \
  --output datasets/oqt_identity_v1.jsonl \
  --size 500

# 2. Fine-tune base models with LoRA
python ml_service/train.py \
  --base-model mistral-7b \
  --dataset datasets/oqt_identity_v1.jsonl \
  --method lora \
  --output models/oqt/weights/lora_adapters/oqt-1.0-v1.0

# 3. Repeat for LLaMA-2
python ml_service/train.py \
  --base-model llama-2-7b \
  --dataset datasets/oqt_identity_v1.jsonl \
  --method lora \
  --output models/oqt/weights/lora_adapters/oqt-1.0-v1.0
```

**Duration**: 2-4 hours on GPU  
**Result**: OQT-1.0 now responds with OpenSeek identity  
**Version**: OQT-1.0.v1.0 (initial release)

#### 3. Continuous Identity Reinforcement

**With every microtraining session**, OQT-1.0 reinforces its identity:

- **Stage 1**: Learn content from external AI responses
- **Stage 2**: Apply OpenSeek perspective (fairness, transparency, multi-model synthesis)

**Identity Markers in Responses**:
```javascript
{
  response: "Jag √§r OpenSeek... [answer]",
  metadata: {
    identity: "OpenSeek AI-agent",
    model: "OQT-1.0.v13.2",
    base_models: ["Mistral 7B", "LLaMA-2"],
    fairness_score: 0.87,
    provenance: "#interaction_2025_11_20_001"
  }
}
```

#### 4. Dashboard Presentation

**In OQT Dashboard**, every response displays:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ü§ñ OpenSeek AI-agent                         ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ [Response text]                              ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ Model: OQT-1.0.v13.2                         ‚îÇ
‚îÇ Confidence: 92%                              ‚îÇ
‚îÇ Fairness: 0.87                               ‚îÇ
‚îÇ Provenance: #interaction_2025_11_20_001      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Ledger Tab** shows complete transparency:
- Which external AIs contributed
- How consensus was calculated
- Training events that shaped this version
- Full provenance chain

### Open Instruction Datasets (Recommended)

For initial training, these open datasets can be used:

**General Instruction Datasets**:
1. **Alpaca** - 52K instruction-following examples
   - Source: `yahma/alpaca-cleaned`
   - License: CC BY-NC 4.0
   
2. **Dolly 15K** - High-quality human-generated
   - Source: `databricks/databricks-dolly-15k`
   - License: CC BY-SA 3.0
   
3. **FLAN Collection** - Multi-task instructions
   - Source: `google/flan-t5-xxl`
   - License: Apache 2.0

**Swedish Language Datasets** (for Swedish OQT):
1. **Nordic LLM** instruction data
2. **Swedish translated Alpaca**
3. Custom OpenSeek Swedish instructions

**Recommended Approach**:
```bash
# 1. Start with general instruction dataset (Alpaca)
# 2. Add OpenSeek-specific identity examples (500 examples)
# 3. Fine-tune with combined dataset
# 4. Continue with real-time microtraining from ai_interactions
```

### Implementation Status

| Component | Status | Notes |
|-----------|--------|-------|
| **LoRA/PEFT Infrastructure** | üîÑ In Progress | Code structure ready, needs implementation |
| **Instruction Dataset** | üìã Planned | Template created, needs 500 examples |
| **Initial Fine-Tuning** | üìã Planned | Waiting for instruction dataset |
| **Real-Time Microtraining** | üîÑ In Progress | Backend hooks ready, training logic needed |
| **Identity Enforcement** | üìã Planned | Depends on initial fine-tuning |
| **LoRA Adapter Storage** | ‚úÖ Complete | Directory structure created |
| **Version Management** | ‚úÖ Complete | Tracking system in place |

### Next Steps

1. **Create Instruction Dataset** (Week 1)
   - Write 500 OpenSeek identity examples
   - Include Swedish and English variants
   - Add fairness/transparency focus

2. **Initial Fine-Tuning** (Week 2)
   - Fine-tune Mistral 7B with LoRA
   - Fine-tune LLaMA-2 with LoRA
   - Test identity responses

3. **Implement Microtraining** (Week 3)
   - Connect ai_interactions to training pipeline
   - Implement Stage 1 & 2 LoRA updates
   - Test version increments

4. **Deploy & Monitor** (Week 4)
   - Launch OQT-1.0.v1.0
   - Monitor identity consistency
   - Track performance metrics

---

## Ledger & Provenance

OQT-1.0 maintains **complete transparency** through blockchain-style ledger.

### Provenance Chain

Every OQT-1.0 response can be traced back through the entire process:

```
Question
  ‚Üì
Raw AI Responses (6 services)
  ‚Üì
ML Pipeline Analysis
  ‚Üì
Training Stage 1 (Raw Data)
  ‚Üì
Training Stage 2 (Analyzed Data)
  ‚Üì
Model Version Update
  ‚Üì
Ledger Block
  ‚Üì
OQT-1.0 Response
```

### What's Logged:

1. **Original Question**
   - Text, timestamp, user

2. **Data Sources**
   - Which AI services responded
   - Response timestamps

3. **Analysis Results**
   - Consensus, bias, fairness scores
   - ML pipeline version

4. **Training Events**
   - Both training stages
   - Model versions before/after
   - Performance changes

5. **Model Updates**
   - Weight file paths
   - Checksums for verification

6. **Response Generation**
   - Which model version answered
   - Confidence score
   - Generation time

### Ledger Storage

**Primary**: `oqt_ledger` collection in Firebase  
**Backup**: Exported to blockchain-style blocks  

Each ledger entry includes:
- Timestamp
- Transaction type
- Data hash
- Previous block reference
- Immutable flag

### Transparency Dashboard

Users can view full provenance in **OQT Dashboard ‚Üí Ledger tab**:
- See which AI services contributed to training
- View analysis metrics that influenced the model
- Trace model version evolution
- Verify data integrity via hashes

---

## OQT Dashboard

The OQT Dashboard (`/oqt-dashboard`) provides real-time interaction and monitoring of OQT-1.0.

### 4 Tabs:

### 1. Chat (Chatt)

**Purpose**: Direct conversation with OQT-1.0

**Features**:
- **Minimal chat interface** with message bubbles
- **User messages**: Light background, right-aligned
- **OQT-1.0 responses**: Dark background with ü§ñ icon, left-aligned
- **Auto-scroll**: Automatically scrolls to latest message
- **Loading animation**: Bouncing dots during inference
- **Confidence score**: Shows OQT-1.0's confidence (0-100%)
- **Input field**: Fixed bottom, matches ChatV2 design

**User Experience**:
```
User: "What is democracy?"
  ‚Üì
OQT-1.0 (93% confident):
"Democracy is a system of government where power is held by the 
people, either directly or through elected representatives..."
```

### 2. Aktivitet (Activity)

**Purpose**: Real-time training activity visualization

**Features** (Planned):
- **Live training log**: Shows each microtraining event as it happens
- **Training stages**: Visual indication of Stage 1 (raw) and Stage 2 (analyzed)
- **Version updates**: Real-time version increments
- **Performance metrics**: Loss improvements, accuracy changes
- **Timeline**: Chronological view of all training events

**Example Display**:
```
üîÑ Training in progress...
‚îú‚îÄ Stage 1: Raw data from 6 AI services
‚îÇ  Model: OQT-1.0.v13.2 ‚Üí v13.3
‚îÇ  Samples: 6 | Duration: 45s | Loss: 0.245 ‚Üí 0.241
‚îÇ
‚îú‚îÄ Stage 2: Analyzed metrics (consensus: 0.95, bias: 2.1)
‚îÇ  Model: OQT-1.0.v13.3 ‚Üí v13.4
‚îÇ  Updates: fairness+0.02, bias_detection+0.01 | Duration: 38s
‚îÇ
‚úÖ Training complete! Model updated to OQT-1.0.v13.4
```

### 3. M√§tv√§rden (Metrics)

**Purpose**: Model performance tracking over time

**Features** (Planned):
- **Performance graphs**: Response time, confidence, accuracy
- **Fairness metrics**: Bias scores, fairness index over time
- **Training statistics**: Total samples, training frequency
- **Comparison charts**: Current vs previous major version
- **Usage stats**: Queries processed, active users

**Example Metrics**:
```
Model Performance (OQT-1.0.v13.4)
‚îú‚îÄ Average Confidence: 89%
‚îú‚îÄ Response Time: 850ms
‚îú‚îÄ User Satisfaction: 4.2/5
‚îú‚îÄ Bias Score: 2.1/10 (Low)
‚îú‚îÄ Fairness Index: 88% (Excellent)
‚îî‚îÄ Queries Today: 45
```

### 4. Ledger

**Purpose**: Transparency and provenance tracking

**Features** (Planned):
- **Blockchain-style ledger**: Immutable transaction log
- **Provenance chains**: Trace any response back to sources
- **Data integrity**: Hash verification for all blocks
- **Audit trail**: Complete history of model updates
- **Search/filter**: Find specific transactions or questions

**Example Ledger Entry**:
```
Block #1523
‚îú‚îÄ Timestamp: 2025-11-20 12:00:00
‚îú‚îÄ Type: Microtraining
‚îú‚îÄ Question: "What is democracy?"
‚îú‚îÄ AI Sources: GPT-4, Gemini, Grok, Claude, DeepSeek, Qwen
‚îú‚îÄ Training: Stage 1 (raw) + Stage 2 (analyzed)
‚îú‚îÄ Version: OQT-1.0.v13.2 ‚Üí v13.4
‚îú‚îÄ Metrics: Consensus 0.95, Bias 2.1, Fairness 0.88
‚îú‚îÄ Hash: sha256:a3f2...
‚îî‚îÄ Previous: sha256:b1e4...
```

---

## Model Weights Storage

### Recommended Directory Structure

```
models/
‚îî‚îÄ‚îÄ oqt/
    ‚îú‚îÄ‚îÄ weights/
    ‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v1.0.pth              # Major version
    ‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v1.0.json             # Metadata
    ‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v1.1.pth              # Micro version
    ‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v1.1.json             # Metadata
    ‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v1.2.pth
    ‚îÇ   ‚îú‚îÄ‚îÄ oqt-1.0-v1.2.json
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îÇ
    ‚îú‚îÄ‚îÄ checkpoints/
    ‚îÇ   ‚îú‚îÄ‚îÄ daily/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-2025-11-20.pth
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îÇ   ‚îî‚îÄ‚îÄ weekly/
    ‚îÇ       ‚îú‚îÄ‚îÄ checkpoint-week-47.pth
    ‚îÇ       ‚îî‚îÄ‚îÄ ...
    ‚îÇ
    ‚îú‚îÄ‚îÄ backups/
    ‚îÇ   ‚îú‚îÄ‚îÄ firebase-storage/             # Cloud backup sync
    ‚îÇ   ‚îî‚îÄ‚îÄ local-backup/
    ‚îÇ
    ‚îî‚îÄ‚îÄ base_models/
        ‚îú‚îÄ‚îÄ mistral-7b/                    # Mistral 7B weights
        ‚îî‚îÄ‚îÄ llama-2-7b/                    # LLaMA-2 weights
```

### File Formats

**Model Weights**:
- **Format**: PyTorch `.pth` or Safetensors `.safetensors`
- **Size**: ~13-14 GB per version (7B parameters)
- **Compression**: None (for fast loading)

**Metadata JSON**:
```json
{
  "version": "OQT-1.0.v13.2",
  "created_at": "2025-11-20T12:00:00Z",
  "base_models": ["mistral-7b", "llama-2-7b"],
  "training": {
    "samples_processed": 15234,
    "last_major_training": "2025-11-13T00:00:00Z",
    "microtraining_events": 1523
  },
  "performance": {
    "average_confidence": 0.89,
    "bias_score": 2.1,
    "fairness_index": 0.88
  },
  "checksum": "sha256:a3f2e1b4...",
  "file_size_bytes": 14256789456
}
```

### Storage Strategy

1. **Local Storage** (Primary):
   - Fast access for inference
   - Keep last 3 major versions
   - Keep last 50 micro versions

2. **Firebase Storage** (Backup):
   - All major versions (permanent)
   - Last 100 micro versions (rolling)
   - Automatic sync after training

3. **Archival**:
   - Major versions archived monthly
   - Compressed for long-term storage
   - Accessible for rollback if needed

### Disk Space Requirements

- **Base models**: ~28 GB (Mistral 7B + LLaMA-2 7B)
- **OQT-1.0 versions**: ~14 GB per version
- **Recommended**: 100-200 GB SSD for local storage
- **Cloud backup**: Managed via Firebase Storage

---

## Implementation Status

### ‚úÖ Fully Implemented

**Backend Services**:
- ‚úÖ `services/mistral.js` - Mistral 7B integration (simulated)
- ‚úÖ `services/llama.js` - LLaMA-2 integration (simulated)
- ‚úÖ `services/oqtMultiModelPipeline.js` - Multi-model orchestration

**API Endpoints**:
- ‚úÖ `/api/oqt/query` - Direct OQT-1.0 queries
- ‚úÖ `/api/oqt/multi-model-query` - Multi-model pipeline

**Frontend**:
- ‚úÖ OQT Dashboard with minimal chat interface
- ‚úÖ 4 tabs: Chat, Aktivitet, M√§tv√§rden, Ledger
- ‚úÖ Chat functionality with message bubbles
- ‚úÖ Auto-scroll and loading animations
- ‚úÖ Input field matching ChatV2 design

**Firebase Integration**:
- ‚úÖ Uses existing `ai_interactions` collection (PR #44)
- ‚úÖ `oqt_queries` collection
- ‚úÖ `oqt_training_events` collection
- ‚úÖ `oqt_metrics` collection
- ‚úÖ `oqt_ledger` collection
- ‚úÖ Ledger services (`ledgerService.js`, `oqtLedgerService.js`)

**Infrastructure**:
- ‚úÖ ML service skeleton (`ml_service/server.py`)
- ‚úÖ Model download script (`scripts/download_models.py`)
- ‚úÖ Firebase setup script (`scripts/setup_firebase.py`)
- ‚úÖ Quick setup automation (`.sh` and `.ps1`)

**Documentation**:
- ‚úÖ Installation guide (`INSTALLATION_GUIDE.md`)
- ‚úÖ API documentation (`docs/OQT_MULTI_MODEL_API.md`)
- ‚úÖ Implementation guide (`OQT_MULTI_MODEL_README.md`)
- ‚úÖ Complete OQT-1.0 README (this document)

**ML Pipeline Libraries**:
- ‚úÖ spaCy - NLP core processing
- ‚úÖ TextBlob - Sentiment analysis  
- ‚úÖ langdetect - Language detection
- ‚úÖ Detoxify - Toxicity scoring
- ‚úÖ Transformers - Model framework
- ‚úÖ Gensim - Topic modeling
- ‚úÖ Fairlearn - Fairness metrics
- ‚úÖ Complete OQT-1.0 documentation (this file)

**Testing**:
- ‚úÖ 14 tests for services and pipeline
- ‚úÖ Frontend build verification

### üîÑ Needs Implementation

**ML Service (Actual Inference)**:
- üîÑ Real Mistral 7B model loading
- üîÑ Real LLaMA-2 model loading
- üîÑ GPU/CPU optimization
- üîÑ Model caching implementation
- üîÑ 8-bit quantization

**Training Pipeline**:
- üîÑ Actual PyTorch training implementation
- üîÑ LoRA/PEFT fine-tuning setup
- üîÑ Stage 1: Raw data fine-tuning
- üîÑ Stage 2: Analyzed data fine-tuning
- üîÑ Large dataset training scheduler
- üîÑ Model weight persistence
- üîÑ Version management automation
- üîÑ Instruction dataset creation (500 OpenSeek identity examples)

**ML Pipeline - Advanced Libraries**:
- üîÑ SHAP - Explainability (partial integration)
- üìã BERTopic - Advanced topic modeling
- üìã LIME - Local explanations
- üìã Lux - Dashboard visualizations
- üìã Sweetviz - Data profiling

**BERT Summarizer Integration**:
- üîÑ BERT Summarizer library integration
- üìã Raw response summarization (Week 1-2)
- üìã Analysis summarization (metaSummary) (Week 2-3)
- ‚úÖ Dashboard UI for summary display (ready)
- üìã Training integration with summaries (Week 3-4)
- ‚úÖ Provenance tracking (complete)

**Dashboard Tabs (Content)**:
- ‚úÖ Chat tab (functional)
- üîÑ Aktivitet tab (placeholder ‚Üí real-time training visualization)
- üîÑ M√§tv√§rden tab (placeholder ‚Üí performance graphs)
- üîÑ Ledger tab (placeholder ‚Üí ledger blockchain view)

**Production Features**:
- üîÑ Model weight backups to Firebase Storage
- üîÑ Automatic rollback on failure
- üîÑ Performance monitoring alerts
- üîÑ Usage analytics
- üîÑ Rate limiting
- üîÑ Caching layer

### üìã Development Roadmap

**Phase 1**: ML Infrastructure (Current)
- Implement actual model loading (Mistral 7B, LLaMA-2)
- GPU optimization and memory management
- Basic inference endpoint

**Phase 2**: Training Pipeline
- Stage 1 microtraining (raw data)
- Stage 2 microtraining (analyzed data)
- Weight persistence and versioning

**Phase 3**: Large Dataset Training
- Weekly/monthly batch training
- Major version management
- Checkpoint system

**Phase 4**: Dashboard Enhancement
- Real-time Aktivitet tab
- Performance M√§tv√§rden graphs
- Ledger blockchain visualization

**Phase 5**: Production Hardening
- Monitoring and alerting
- Backup and recovery
- Performance optimization
- Security hardening

---

## API Endpoints

### 1. `/api/oqt/query`

**Method**: `POST`  
**Purpose**: Direct OQT-1.0 inference (used by dashboard)

**Request**:
```json
{
  "question": "What is democracy?",
  "user_id": "user-123" // optional
}
```

**Response**:
```json
{
  "response": "Democracy is a system of government...",
  "confidence": 0.93,
  "model_version": "OQT-1.0.v13.4",
  "base_models": ["mistral-7b", "llama-2-7b"],
  "generation_time_ms": 856,
  "provenance": {
    "training_sources": ["gpt4", "gemini", "grok"],
    "ledger_block": "ledger-hash-123"
  }
}
```

### 2. `/api/oqt/multi-model-query`

**Method**: `POST`  
**Purpose**: Multi-model pipeline for training data collection

**Request**:
```json
{
  "question": "What is democracy?",
  "includeExternal": true,
  "enableTraining": true
}
```

**Response**:
```json
{
  "response": "OQT-1.0 synthesized response...",
  "confidence": 0.92,
  "model_version": "OQT-1.0.v13.4",
  
  "external_responses": [
    {
      "service": "gpt4",
      "response": "Democracy is...",
      "latency_ms": 1234
    },
    // ... 5 more services
  ],
  
  "analysis": {
    "consensus": {
      "score": 0.95,
      "level": "high",
      "agreements": ["Democracy involves voting"],
      "disagreements": []
    },
    "bias": {
      "aggregated_score": 2.1,
      "level": "low",
      "types": []
    },
    "fairness": {
      "score": 0.88,
      "level": "excellent"
    }
  },
  
  "training": {
    "stage1": {
      "status": "completed",
      "samples_processed": 6,
      "model_updated": "OQT-1.0.v13.2 ‚Üí v13.3"
    },
    "stage2": {
      "status": "completed",
      "metrics_updated": ["fairness", "bias_detection"],
      "model_updated": "OQT-1.0.v13.3 ‚Üí v13.4"
    }
  },
  
  "ledger_block": "ledger-hash-123"
}
```

### 3. `/api/oqt/status`

**Method**: `GET`  
**Purpose**: Get current model status

**Response**:
```json
{
  "model_version": "OQT-1.0.v13.4",
  "status": "ready",
  "base_models": {
    "mistral-7b": "loaded",
    "llama-2-7b": "loaded"
  },
  "last_training": "2025-11-20T12:00:00Z",
  "queries_today": 45,
  "uptime_seconds": 86400
}
```

### 4. `/api/oqt/metrics`

**Method**: `GET`  
**Purpose**: Get performance metrics

**Response**:
```json
{
  "current_version": "OQT-1.0.v13.4",
  "performance": {
    "average_confidence": 0.89,
    "average_response_time_ms": 850,
    "user_satisfaction": 4.2
  },
  "fairness": {
    "bias_score": 2.1,
    "fairness_index": 0.88
  },
  "training": {
    "total_samples": 15234,
    "microtraining_events_today": 45
  }
}
```

---

## Firebase Collections (Aktuell Anv√§ndning)

OQT-1.0 anv√§nder **6 collections** i Firebase Firestore. Redundanta collections har tagits bort baserat p√• faktisk anv√§ndning i koden.

### Aktiva Collections:

#### 1. **`ai_interactions`**
- **Syfte**: Unified lagring av fr√•gor, r√•svar fr√•n externa AI-tj√§nster, och ML-pipeline-analyser
- **Datatyp**: Dokument med nested objekt
- **Schema**:
  ```javascript
  {
    interactionId: "auto-generated",
    question: {
      text: "Anv√§ndarens fr√•ga",
      timestamp: "ISO timestamp",
      userId: "valfritt",
      source: "start_view | oqt_dashboard"
    },
    raw_responses: [
      {
        service: "gpt4 | gemini | grok | claude | deepseek | qwen",
        response: "AI-svar text",
        timestamp: "ISO timestamp",
        latency_ms: 123,
        tokens: 150,
        model_version: "gpt-4-turbo"
      }
    ],
    processed_data: {
      consensus: { score: 0.95, level: "high", metrics: {} },
      bias: { aggregated_score: 2.1, level: "low", types: [] },
      fairness: { score: 0.88, level: "excellent" },
      meta_summary: { ... }
    },
    timestamp: "ISO timestamp"
  }
  ```
- **Anv√§ndning**: Central datak√§lla f√∂r tr√§ning, analys och transparens

#### 2. **`oqt_queries`**
- **Syfte**: Direkta fr√•gor till OQT-1.0 fr√•n dashboard
- **Datatyp**: Dokument
- **Schema**:
  ```javascript
  {
    queryId: "auto-generated",
    question: "Anv√§ndarens fr√•ga",
    response: "OQT-1.0 svar",
    confidence: 0.92,
    timestamp: "ISO timestamp",
    model: "OQT-1.0",
    version: "1.2.0",
    metadata: { tokens: 150, latency_ms: 850, modelsUsed: ["mistral", "llama"] }
  }
  ```
- **Anv√§ndning**: Sp√•rar anv√§ndarinteraktioner med OQT-1.0

#### 3. **`oqt_training_events`**
- **Syfte**: Loggning av tr√§ningssessioner (micro-training och batch training)
- **Datatyp**: Dokument
- **Schema**:
  ```javascript
  {
    trainingId: "auto-generated",
    type: "micro-training | batch-training | weekly-training",
    timestamp: "ISO timestamp",
    samplesProcessed: 6,
    stage1: { method: "raw_response_training", samplesProcessed: 6, updated: true },
    stage2: { method: "analyzed_data_training", metricsUpdated: true },
    modelVersion: "1.2.0",
    metrics: { accuracy: 0.91, fairness: 0.88, bias: 2.1, consensus: 0.95 }
  }
  ```
- **Anv√§ndning**: Transparens kring modelltr√§ning

#### 4. **`oqt_metrics`**
- **Syfte**: Prestationsmetriker f√∂r OQT-1.0 √∂ver tid
- **Datatyp**: Dokument
- **Schema**:
  ```javascript
  {
    metricId: "auto-generated",
    version: "1.2.0",
    timestamp: "ISO timestamp",
    metrics: { accuracy: 0.91, fairness: 0.88, bias: 2.1, consensus: 0.95 },
    training: { totalSamples: 15234, weeklyBatches: 12, microBatches: 1523 }
  }
  ```
- **Anv√§ndning**: Dashboard "M√§tv√§rden" tab

#### 5. **`oqt_provenance`**
- **Syfte**: Provenienshantering f√∂r transparens
- **Datatyp**: Dokument
- **Schema**:
  ```javascript
  {
    provenanceId: "auto-generated",
    queryId: "referens till oqt_queries",
    timestamp: "ISO timestamp",
    model: "OQT-1.0",
    version: "1.2.0",
    processingSteps: [
      { step: "tokenization", timestamp: "ISO timestamp" },
      { step: "inference", timestamp: "ISO timestamp" }
    ],
    inputHash: "hash av input"
  }
  ```
- **Anv√§ndning**: Fullst√§ndig sp√•rbarhet av beslut

#### 6. **`oqt_ledger`**
- **Syfte**: Blockchain-stil immutable ledger
- **Datatyp**: Dokument
- **Schema**:
  ```javascript
  {
    blockNumber: 1523,
    type: "query | training | update",
    timestamp: "ISO timestamp",
    data: { queryId: "...", trainingId: "...", description: "..." },
    hash: "SHA256 hash av block",
    previousHash: "hash av f√∂reg√•ende block"
  }
  ```
- **Anv√§ndning**: Orubblig logg f√∂r full transparens

### Borttagna Collections (Redundanta):

F√∂ljande collections har tagits bort fr√•n `setup_firebase.py` eftersom deras data redan finns i befintliga collections:

- ‚ùå **`questions`** ‚Üí Data finns i `ai_interactions.question`
- ‚ùå **`external_raw_responses`** ‚Üí Data finns i `ai_interactions.raw_responses[]`
- ‚ùå **`per_response_analysis`** ‚Üí Data finns i `ai_interactions.processed_data`
- ‚ùå **`oqt_model_versions`** ‚Üí Kan h√§rledas fr√•n `oqt_training_events`
- ‚ùå **`ledger_entries`** ‚Üí Duplikat av `oqt_ledger`

---

## API Endpoints Status

### OQT-1.0 Core Endpoints

| Endpoint | Method | Status | Beskrivning |
|----------|--------|--------|-------------|
| `/api/oqt/query` | POST | ‚úÖ UP | Generera svar fr√•n OQT-1.0 (simulerat) |
| `/api/oqt/multi-model-query` | POST | ‚úÖ UP | Multi-model pipeline (Mistral + LLaMA + analys) |
| `/api/oqt/micro-train` | POST | ‚úÖ UP | Real-time micro-training (tv√•-stegs) |
| `/api/oqt/train` | POST | ‚úÖ UP | Veckovis batch-tr√§ning (simulerat) |
| `/api/oqt/status` | GET | ‚úÖ UP | Modellstatus och h√§lsa |
| `/api/oqt/metrics` | GET | ‚úÖ UP | Prestationsmetriker |
| `/api/oqt/ledger/verify` | GET | ‚úÖ UP | Verifiera ledger-integritet |
| `/api/oqt/ledger/stats` | GET | ‚úÖ UP | Ledger-statistik |

### ML Service Endpoints (Port 5000)

| Endpoint | Method | Status | Beskrivning |
|----------|--------|--------|-------------|
| `/` | GET | ‚úÖ UP | H√§lsokontroll |
| `/inference/mistral` | POST | üîÑ SKELETON | Mistral 7B inferens (kr√§ver nedladdad modell) |
| `/inference/llama` | POST | üîÑ SKELETON | LLaMA-2 inferens (kr√§ver nedladdad modell) |
| `/models/status` | GET | ‚úÖ UP | Status f√∂r laddade modeller |

### Endpoint Status-f√∂rklaring:

- ‚úÖ **UP**: Fullt funktionell (kan vara simulerad)
- üîÑ **SKELETON**: Kodskelett finns, kr√§ver modellnedladdning f√∂r verklig inferens
- ‚ö†Ô∏è **PARTIAL**: Delvis implementerad
- ‚ùå **DOWN**: Ej implementerad/fungerar inte

### Testa Endpoints:

```bash
# Testa OQT query
curl -X POST http://localhost:3001/api/oqt/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Vad √§r demokrati?"}'

# Testa multi-model query
curl -X POST http://localhost:3001/api/oqt/multi-model-query \
  -H "Content-Type: application/json" \
  -d '{"question": "Vad √§r AI?", "includeExternal": false, "enableTraining": true}'

# Testa ML service (kr√§ver nedladdade modeller)
curl -X POST http://localhost:5000/inference/mistral \
  -H "Content-Type: application/json" \
  -d '{"text": "Vad √§r AI?", "max_length": 256}'

# Kontrollera status
curl http://localhost:3001/api/oqt/status
curl http://localhost:5000/
```

---

## Implementation Status & Integration

### ‚úÖ Fullt Implementerat

**Backend Services**:
- ‚úÖ `services/mistral.js` - Mistral 7B integration (simulerad tills modell laddas)
- ‚úÖ `services/llama.js` - LLaMA-2 integration (simulerad tills modell laddas)
- ‚úÖ `services/oqtMultiModelPipeline.js` - Multi-model orkestrering

**API Endpoints**:
- ‚úÖ `/api/oqt/query` - Direkt OQT-1.0 fr√•gor
- ‚úÖ `/api/oqt/multi-model-query` - Multi-model pipeline med analys
- ‚úÖ `/api/oqt/micro-train` - Real-time micro-training
- ‚úÖ `/api/oqt/train` - Batch training
- ‚úÖ Alla status/metrics endpoints

**Frontend**:
- ‚úÖ OQT Dashboard (`/oqt-dashboard`)
- ‚úÖ Chat-funktionalitet med meddelandebubblor
- ‚úÖ 4 flikar: Chat, Aktivitet, M√§tv√§rden, Ledger
- ‚úÖ Auto-scroll och laddningsanimationer

**Firebase Integration**:
- ‚úÖ 6 centrala collections (se ovan)
- ‚úÖ Ledger services (`ledgerService.js`, `oqtLedgerService.js`)
- ‚úÖ Firebase service (`oqtFirebaseService.js`)

**Infrastruktur**:
- ‚úÖ ML service skeleton (`ml_service/server.py`)
- ‚úÖ Modellnedladdningsskript (`scripts/download_models.py`)
- ‚úÖ Firebase setup script (`scripts/setup_firebase.py`)
- ‚úÖ Snabbinstallation (`.sh` och `.ps1`)

**Dokumentation**:
- ‚úÖ Installationsguide (`INSTALLATION_GUIDE.md`)
- ‚úÖ API-dokumentation (`docs/OQT_MULTI_MODEL_API.md`)
- ‚úÖ Komplett OQT-1.0 README (detta dokument)

### üîÑ Kr√§ver Modellnedladdning

**ML Service (Verklig Inferens)**:
- üîÑ Mistral 7B modell laddning och inferens
- üîÑ LLaMA-2 modell laddning och inferens
- üîÑ GPU/CPU optimering
- üîÑ 8-bit quantization

**Tr√§ningspipeline**:
- üîÑ PyTorch-baserad tr√§ning
- üîÑ LoRA/PEFT fine-tuning
- üîÑ Stage 1: R√•datatr√§ning
- üîÑ Stage 2: Analyserad datatr√§ning
- üîÑ Modellversionering

### K√∂ra Systemet

**1. Simulerat L√§ge (Fungerar Nu)**:
```bash
# Terminal 1: Backend
cd backend && npm run dev

# Terminal 2: Frontend
cd frontend && npm run dev

# Terminal 3: ML Service (optional - skeleton)
python ml_service/server.py

# √ñppna: http://localhost:3000/oqt-dashboard
```

**Status**: ‚úÖ Alla endpoints fungerar med simulerade svar

**2. Verkligt L√§ge (Kr√§ver Modellnedladdning)**:
```bash
# 1. Ladda ner modeller
python scripts/download_models.py

# 2. K√∂r samma som ovan
# ML service kommer nu anv√§nda verkliga modeller
```

**Status**: üîÑ Kr√§ver ~27GB modellfiler (Mistral 7B + LLaMA-2)

### Verklig vs Simulerad Inferens

| Komponent | Simulerat (Nu) | Verkligt (Efter Nedladdning) |
|-----------|----------------|------------------------------|
| **Mistral 7B** | ‚úÖ F√∂rutbest√§mda svar | üîÑ Verklig transformer-inferens |
| **LLaMA-2** | ‚úÖ F√∂rutbest√§mda svar | üîÑ Verklig transformer-inferens |
| **Pipeline** | ‚úÖ Fungerar fullt | ‚úÖ Samma (analyspipeline) |
| **Tr√§ning** | ‚úÖ Simulerad metricsuppdatering | üîÑ Verklig LoRA fine-tuning |
| **Dashboard** | ‚úÖ Fullt funktionell | ‚úÖ Samma |
| **API** | ‚úÖ Alla endpoints | ‚úÖ Samma |

### N√§sta Steg f√∂r Full Implementation

1. **Ladda Ner Modeller** (27GB totalt):
   ```bash
   python scripts/download_models.py
   ```

2. **Verifiera Modellfiler**:
   ```bash
   ls -lh models/mistral-7b-instruct/
   ls -lh models/llama-2-7b-chat/
   ```

3. **Implementera Verklig Tr√§ning**:
   - LoRA adapters f√∂r Mistral 7B
   - LoRA adapters f√∂r LLaMA-2
   - Stage 1 & 2 micro-training

4. **Optimering**:
   - GPU acceleration
   - Model caching
   - 8-bit quantization

---

## Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- 16GB RAM minimum (32GB recommended)
- 50GB disk space minimum
- NVIDIA GPU with 12GB+ VRAM (recommended)

### Installation

**Automated (Recommended)**:

```bash
# Linux/Mac
./scripts/quick_setup.sh

# Windows PowerShell
.\scripts\quick_setup.ps1
```

**Manual**:

```bash
# 1. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
.\venv\Scripts\Activate.ps1  # Windows

# 2. Install Python dependencies
pip install -r requirements.txt

# 3. Download base models
python scripts/download_models.py

# 4. Setup Firebase
python scripts/setup_firebase.py

# 5. Configure environment
cp backend/.env.example backend/.env
cp frontend/.env.example frontend/.env
# Edit .env files with your Firebase credentials

# 6. Install Node.js dependencies
cd backend && npm install
cd ../frontend && npm install
```

### Running the Application

**Terminal 1 - ML Service**:
```bash
source venv/bin/activate  # Activate venv
python ml_service/server.py
# Runs on http://localhost:5000
```

**Terminal 2 - Backend**:
```bash
cd backend
npm run dev
# Runs on http://localhost:3001
```

**Terminal 3 - Frontend**:
```bash
cd frontend
npm run dev
# Runs on http://localhost:3000
```

**Access OQT Dashboard**:
```
http://localhost:3000/oqt-dashboard
```

---

## Performance Metrics

### Current Performance (Simulated)

| Metric | Value | Target |
|--------|-------|--------|
| **Response Time** | ~850ms | <1s |
| **Confidence** | 89% avg | >90% |
| **Bias Score** | 2.1/10 | <3.0 |
| **Fairness Index** | 88% | >85% |
| **Training Time** | ~90s/question | <60s |
| **GPU Memory** | ~8GB | <12GB |

### Expected Performance (With Real Models)

| Metric | Value | Notes |
|--------|-------|-------|
| **Response Time** | 1-2s | With GPU |
| **Response Time** | 5-10s | CPU only |
| **Confidence** | 90-95% | After training |
| **Bias Score** | <2.0 | With improvements |
| **Fairness Index** | >90% | Goal |
| **Queries/hour** | 1000+ | With caching |

---

## Summary

**OneSeek-7B-Zero** (formerly OQT-1.0) is an independent, transparent, continuously-learning language model built on **Mistral 7B** and **LLaMA-2** foundations. It learns from 6 external AI services through a sophisticated two-step microtraining process, maintains complete transparency via blockchain-style ledger, and provides users with fair, unbiased, traceable responses.

**Key Differentiators**:
- ‚úÖ Own model (not just API wrapper)
- ‚úÖ Real-time learning on every question
- ‚úÖ Full transparency & provenance
- ‚úÖ Bias detection & fairness optimization
- ‚úÖ User-friendly dashboard interface
- ‚úÖ **Identity training with LoRA/PEFT for OpenSeek AI-agent personality**

---

## Phase 2 Completion Status (November 2025)

### ‚úÖ Completed in This Phase

**1. Model Identity & Naming**:
- ‚úÖ Renamed from OQT-1.0 to OneSeek-7B-Zero
- ‚úÖ Implemented versioning: `OneSeek-7B-Zero.v{MAJOR}.{MICRO}`
- ‚úÖ Maintained backward compatibility for all OQT references
- ‚úÖ Updated all documentation and code references

**2. Instruction Dataset & Identity Training**:
- ‚úÖ Created identity dataset: `datasets/oneseek_identity_v1.jsonl` (74 bilingual examples)
- ‚úÖ Covers: Identity, training process, transparency, fairness, bias detection, versioning
- ‚úÖ Bilingual support (Swedish/English) for OpenSeek identity
- ‚úÖ Dataset integrated into training pipeline structure

**3. Training Pipeline Framework**:
- ‚úÖ PyTorch trainer implementation: `ml/training/train_language_model.py`
- ‚úÖ OneSeekTrainer class with LoRA/PEFT configuration
- ‚úÖ Auto-detection of PyTorch, Transformers, PEFT libraries
- ‚úÖ Real LoRA/PEFT training when dependencies available
- ‚úÖ Automatic fallback to simulation mode when dependencies missing
- ‚úÖ Two-stage training architecture (Stage 1: raw data, Stage 2: analyzed metrics)

**4. Model Storage Structure**:
- ‚úÖ Complete directory hierarchy established
- ‚úÖ Proper naming convention: `oneseek-7b-zero-v{MAJOR}.{MICRO}.pth/json`
- ‚úÖ Supports both new structure (`models/oneseek-7b-zero/`) and legacy paths
- ‚úÖ Auto-detects existing models at `models/mistral-7b-instruct/` and `models/llama-2-7b-chat/`
- ‚úÖ LoRA adapter storage: `models/oneseek-7b-zero/lora_adapters/`
- ‚úÖ Checkpoints: daily/ and weekly/ subdirectories
- ‚úÖ Backups: firebase-storage/ and local-backup/

**5. PyTorch Training Implementation**:
- ‚úÖ Real PyTorch/LoRA training module: `ml/training/pytorch_trainer.py`
- ‚úÖ Smart base model detection (multiple location search)
- ‚úÖ GPU/CPU auto-detection
- ‚úÖ 8-bit quantization support
- ‚úÖ Robust error handling with multiple fallback strategies
- ‚úÖ Tokenizer loading with automatic recovery from symlink issues
- ‚úÖ Dependency version management (protobuf==3.20.3)

**6. Quick-Start Training Script**:
- ‚úÖ One-command training: `python scripts/train_identity.py`
- ‚úÖ Automatic dataset verification and conversion
- ‚úÖ Integrated data preparation pipeline
- ‚úÖ PyTorch auto-detection with clear status reporting
- ‚úÖ Progress reporting and error messages
- ‚úÖ Works with existing model installations

**7. Complete Documentation**:
- ‚úÖ Main README with OneSeek-7B-Zero section and 11-step training guide
- ‚úÖ OQT-1.0-README.md updated with new identity and backward compatibility notes
- ‚úÖ SNABBSTART_TR√ÑNING.md - Swedish quick-start guide with protobuf fix
- ‚úÖ ml/training/PYTORCH_TRAINING.md - Complete PyTorch setup guide
- ‚úÖ ONESEEK_7B_ZERO_MIGRATION_GUIDE.md - Migration documentation
- ‚úÖ models/oneseek-7b-zero/MODEL_STORAGE_STRUCTURE.md - Storage documentation

**8. Bug Fixes & Robustness**:
- ‚úÖ Fixed tokenizer loading errors (symlink issues on Windows)
- ‚úÖ Fixed protobuf dependency conflicts (version pinning)
- ‚úÖ Fixed model path detection (supports multiple locations)
- ‚úÖ Auto-recovery from loading failures
- ‚úÖ Clear error messages with exact fix instructions

### üéØ Training Status

**Identity Training Working!**
- ‚úÖ PyTorch training pipeline operational: `python scripts/train_identity.py`
- ‚úÖ Uses existing Mistral 7B and LLaMA-2 models from `models/mistral-7b-instruct/` and `models/llama-2-7b-chat/`
- ‚úÖ LoRA/PEFT parameter-efficient fine-tuning (~0.1% of parameters trainable)
- ‚úÖ Saves model versions with proper naming: `oneseek-7b-zero-v1.0.pth`
- ‚úÖ Transparency ledger integration
- ‚úÖ Full metadata and provenance tracking

**Training Output Structure**:
```
models/oneseek-7b-zero/
‚îú‚îÄ‚îÄ weights/
‚îÇ   ‚îú‚îÄ‚îÄ oneseek-7b-zero-v1.0.pth         # Full model state
‚îÇ   ‚îî‚îÄ‚îÄ oneseek-7b-zero-v1.0.json        # Metadata with provenance
‚îú‚îÄ‚îÄ lora_adapters/
‚îÇ   ‚îî‚îÄ‚îÄ oneseek-7b-zero-v1.0/
‚îÇ       ‚îú‚îÄ‚îÄ adapter_config.json           # LoRA configuration
‚îÇ       ‚îî‚îÄ‚îÄ adapter_model.bin             # LoRA weights (~50-100MB)
‚îî‚îÄ‚îÄ ml/ledger/ledger.json                 # Transparency log
```

---

## Next Development Phase: Admin Dashboard for OpenSeek

### üìã Phase 3 Objectives

**Purpose**: Create a dedicated admin dashboard for managing OneSeek-7B-Zero training and dataset operations.

**Key Features**:

**1. Dataset Management**:
- Upload new training datasets (JSONL format)
- Browse and preview existing datasets
- Validate dataset format and quality
- Edit dataset entries inline
- Version control for datasets

**2. Training Control Panel**:
- Select dataset for training
- Configure training parameters (epochs, batch size, learning rate)
- Start/stop training sessions
- Monitor real-time training progress
- View training logs and metrics

**3. Model Management**:
- List all model versions
- View model metadata and performance metrics
- Download model weights and LoRA adapters
- Compare versions side-by-side
- Rollback to previous versions

**4. Real-time Monitoring**:
- Live training progress (loss, accuracy)
- GPU/CPU utilization graphs
- Training time estimates
- Resource usage tracking

**5. Automation & Scheduling**:
- Schedule periodic training (weekly/monthly batch training)
- Auto-training on new datasets
- Notification system for training completion
- Automatic backup to Firebase Storage

**Frontend Location**: `/admin/oneseek-dashboard` or `/oneseek-admin`

**Technology Stack**:
- React with existing UI components
- Real-time updates via Firebase listeners
- Chart.js or Recharts for visualizations
- File upload with drag-and-drop

**Benefits**:
- ‚úÖ No command-line needed for training
- ‚úÖ Non-technical users can train models
- ‚úÖ Visual feedback for all operations
- ‚úÖ Centralized model management
- ‚úÖ Easy dataset experimentation

**Timeline**: Next milestone after Phase 2 completion

---

## Phase 3: Admin Dashboard for OneSeek-7B-Zero (‚úÖ COMPLETED)

### Overview

The **Admin Dashboard** provides a comprehensive web interface for managing OneSeek-7B-Zero training and dataset operations. Located at `/admin`, it matches the graphical profile and UI/UX of `/api-docs` and `/oqt-dashboard`.

### Features Implemented

#### 1. **Dataset Management** ‚úÖ
- **Upload Interface**: Drag-and-drop file upload supporting JSONL and JSON formats
- **File Validation**: Real-time validation of dataset format and quality
- **Dataset Browser**: List all uploaded datasets with metadata (entries count, size, upload date)
- **Preview Modal**: View dataset contents in a formatted JSON preview
- **Quality Metrics**: Display validation results showing valid/invalid entries
- **Delete Functionality**: Remove unwanted datasets

**Key Components:**
- Drag-and-drop upload zone with visual feedback
- Automatic file validation on upload
- Dataset preview with syntax highlighting
- Error reporting for invalid entries

#### 2. **Training Control Panel** ‚úÖ
- **Dataset Selection**: Choose from available datasets for training
- **Parameter Configuration**: Adjust epochs, batch size, and learning rate
- **Training Controls**: Start/stop training sessions
- **Real-time Status**: Monitor training progress, current epoch, and loss
- **Progress Bar**: Visual representation of training completion
- **Training Logs**: Live log viewer showing training events

**Training Parameters:**
- Epochs: 1-100 (default: 3)
- Batch Size: 1-64 (default: 8)
- Learning Rate: 0.00001-0.01 (default: 0.0001)

#### 3. **Model Management** ‚úÖ
- **Version List**: Display all model versions with metadata
- **Current Model Indicator**: Highlight the active model version
- **Model Comparison**: Side-by-side comparison of up to 2 model versions
- **Download Options**: Download model weights and LoRA adapters
- **Rollback Functionality**: Restore previous model versions
- **Detailed Metrics**: View loss, accuracy, fairness, and other performance metrics

**Model Information:**
- Version identifier (e.g., OneSeek-7B-Zero.v1.0)
- Creation timestamp
- Training type (initial, micro-training, batch)
- Samples processed
- Performance metrics

#### 4. **Monitoring Dashboard** ‚úÖ
- **Resource Charts**: Real-time CPU and GPU usage visualization using Chart.js
- **Training History**: List of recent training sessions with metrics
- **Notifications System**: Alert users about training completion and events
- **Training Scheduler**: Configure periodic and automatic training
- **Live Updates**: Polling-based real-time data refresh (5-second intervals)

**Monitoring Features:**
- CPU/GPU usage line charts (last 50 data points)
- Training schedule configuration (manual, daily, weekly, monthly)
- Auto-train on new data toggle
- Notification management with dismiss functionality

### UI/UX Design

The admin dashboard follows the established OneSeek design language:

**Color Scheme:**
- Background: `#0a0a0a` (dark black)
- Panels: `#111` (dark gray)
- Borders: `#2a2a2a` (medium gray)
- Text: `#eee` (primary), `#888` (secondary), `#666` (tertiary)
- Accents: Minimalist borders and subtle hover effects

**Typography:**
- Font: Monospace (system font stack)
- Sizes: 10-18px for various UI elements
- Consistent spacing and alignment

**Layout:**
- Tab-based navigation (Datasets, Training, Models, Monitoring)
- Responsive grid layouts
- Modal dialogs for detailed views
- Fixed headers with consistent branding

### Access Control

**Admin Authentication:**
- Route protected by admin role check
- User role stored in localStorage (`oneseek_user`)
- Access denied page for non-admin users
- Graceful redirect to homepage

**Note:** Current implementation uses simplified client-side role checking. For production, implement server-side authentication middleware.

### Backend API Endpoints

All admin endpoints are prefixed with `/api/admin`:

**Dataset Management:**
- `GET /api/admin/datasets` - List all datasets
- `POST /api/admin/datasets/upload` - Upload new dataset (multipart/form-data)
- `GET /api/admin/datasets/:id/validate` - Validate dataset format
- `DELETE /api/admin/datasets/:id` - Delete dataset

**Training Control:**
- `GET /api/admin/training/status` - Get current training status
- `POST /api/admin/training/start` - Start training session
- `POST /api/admin/training/stop` - Stop training

**Model Management:**
- `GET /api/admin/models` - List all model versions
- `GET /api/admin/models/:id/download?type=weights|lora` - Download model
- `POST /api/admin/models/:id/rollback` - Rollback to model version

**Monitoring:**
- `GET /api/admin/monitoring/resources` - Get CPU/GPU metrics
- `GET /api/admin/monitoring/training-history` - Get training history
- `GET /api/admin/monitoring/schedule` - Get training schedule
- `POST /api/admin/monitoring/schedule` - Update training schedule
- `GET /api/admin/monitoring/notifications` - Get notifications
- `DELETE /api/admin/monitoring/notifications/:id` - Clear notification

### Firebase Integration

The admin dashboard integrates with existing Firebase collections:

**Collections Used:**
- `oqt_training_events` - Training session logs
- `oqt_queries` - Model inference queries
- `oqt_ledger` - Immutable training and query ledger
- `oqt_provenance` - Provenance tracking
- `oqt_metrics` - Model performance metrics

**Real-time Updates:**
- Polling-based updates (5-second intervals)
- Can be enhanced with Firebase real-time listeners

### File Upload & Validation

**Supported Formats:**
- JSONL (JSON Lines) - Preferred format
- JSON (Array of objects)

**Validation Rules:**
- Each line/entry must be valid JSON
- Maximum file size: 100MB
- Automatic error reporting with line numbers

**Example Valid JSONL Entry:**
```jsonl
{"instruction": "Who are you?", "input": "", "output": "I am OpenSeek AI-agent..."}
{"instruction": "What is democracy?", "input": "", "output": "Democracy is..."}
```

### Training Workflow

1. **Upload Dataset**: Drag-and-drop or browse to upload JSONL file
2. **Validate**: System automatically validates format and counts entries
3. **Configure**: Set training parameters (epochs, batch size, learning rate)
4. **Start Training**: Click "Start Training" button
5. **Monitor**: Watch real-time progress, logs, and metrics
6. **Completion**: Receive notification when training finishes
7. **Review**: Check model metrics and compare with previous versions

### Technology Stack

**Frontend:**
- React 18+ with hooks
- React Router for navigation
- Chart.js + react-chartjs-2 for visualizations
- Tailwind CSS for styling
- Fetch API for backend communication

**Backend:**
- Express.js REST API
- Multer for file upload handling
- In-memory state (can be replaced with database)
- Firebase integration (existing infrastructure)

**Charts:**
- Chart.js configured with dark theme
- Responsive line charts for metrics
- Monospace font labels
- Custom color scheme matching UI

### Usage Instructions

**Access the Dashboard:**
1. Navigate to `http://localhost:3000/admin` (or your deployment URL)
2. Ensure you have admin privileges (role: "admin" or isAdmin: true in user object)

**Upload a Dataset:**
1. Go to "Datasets" tab
2. Drag JSONL file to upload zone or click "Browse Files"
3. Review validation results
4. Dataset appears in list below

**Start Training:**
1. Go to "Training" tab
2. Select a dataset from dropdown
3. Configure parameters (epochs, batch size, learning rate)
4. Click "Start Training"
5. Monitor progress in real-time

**Manage Models:**
1. Go to "Models" tab
2. View all model versions
3. Click "Details" to see full metadata
4. Download weights or LoRA adapters
5. Rollback to previous version if needed

**Monitor System:**
1. Go to "Monitoring" tab
2. View CPU/GPU usage charts
3. Check training history
4. Configure training schedule
5. Manage notifications

### Benefits

‚úÖ **No Command Line Required**: Entire training workflow accessible via web UI  
‚úÖ **Visual Feedback**: Real-time charts, progress bars, and status indicators  
‚úÖ **User-Friendly**: Drag-and-drop uploads, intuitive controls  
‚úÖ **Transparency**: Full visibility into training process and model versions  
‚úÖ **Accessible**: Non-technical users can train and manage models  
‚úÖ **Centralized**: All operations in one dashboard  
‚úÖ **Version Control**: Easy comparison and rollback  
‚úÖ **Monitoring**: Resource usage and training history tracking  

### Future Enhancements

**Planned Features:**
- [ ] Firebase real-time listeners for live updates (replace polling)
- [ ] Advanced dataset editing (inline edit, search, filter)
- [ ] Model comparison diff view
- [ ] Training queue management
- [ ] Automatic backup to Firebase Storage
- [ ] Email/push notifications
- [ ] Advanced resource monitoring (memory, disk)
- [ ] Training cost estimation
- [ ] Model performance benchmarking
- [ ] Export training reports
- [ ] Multi-user collaboration features

### Implementation Status

| Feature | Status | Notes |
|---------|--------|-------|
| Dataset Upload | ‚úÖ Complete | Drag-and-drop, validation |
| Dataset Management | ‚úÖ Complete | List, preview, delete |
| Training Control | ‚úÖ Complete | Start/stop, parameters |
| Training Monitoring | ‚úÖ Complete | Progress, logs, metrics |
| Model Listing | ‚úÖ Complete | All versions with metadata |
| Model Comparison | ‚úÖ Complete | Side-by-side 2 models |
| Model Download | üîÑ Partial | API ready, needs file streaming |
| Model Rollback | üîÑ Partial | API ready, needs implementation |
| CPU/GPU Charts | ‚úÖ Complete | Real-time visualization |
| Training History | ‚úÖ Complete | Recent sessions list |
| Notifications | ‚úÖ Complete | Alert system |
| Training Schedule | ‚úÖ Complete | Periodic, auto-train config |
| Access Control | ‚úÖ Complete | Admin role check |
| Firebase Integration | üîÑ Partial | Using existing collections |

---

## Next Development Phase: Advanced Features

**Timeline**: Next milestone after Phase 3 completion

**Current Status**: **Phase 2 Complete!** ‚úÖ Training pipeline fully operational with real PyTorch/LoRA training.

**Phase 3 Status**: **Admin Dashboard Implemented!** ‚úÖ Web-based training and model management interface.

**Next Steps**: Begin Phase 4 - Advanced monitoring and automation features

---

**For more information**:
- Installation: See `INSTALLATION_GUIDE.md`
- API Reference: See `docs/OQT_MULTI_MODEL_API.md`
- Implementation: See `OQT_MULTI_MODEL_README.md`
- Training Guide: See `SNABBSTART_TR√ÑNING.md` (Swedish) or `README.md` (English)
- PyTorch Setup: See `ml/training/PYTORCH_TRAINING.md`
